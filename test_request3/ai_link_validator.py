import os
import json
import re
from urllib.parse import urlparse, urljoin
from datetime import datetime
import openai
from bs4 import BeautifulSoup

# å¯¼å…¥prompté…ç½®
try:
    from backend.ds_prompt_config import LINK_VALIDATION_PROMPT, BATCH_LINK_VALIDATION_PROMPT
except ImportError:
    try:
        from ds_prompt_config import LINK_VALIDATION_PROMPT, BATCH_LINK_VALIDATION_PROMPT
    except ImportError:
        # é»˜è®¤prompté…ç½®
        LINK_VALIDATION_PROMPT = """
        ä½ æ˜¯ä¸€ä¸ªä¸“é—¨åˆ¤æ–­é“¾æ¥æ˜¯å¦ä¸ºæœ‰æ•ˆå†œä¸šæ–°é—»é“¾æ¥çš„AIåŠ©æ‰‹ã€‚è¯·è¯„ä¼°ä»¥ä¸‹é“¾æ¥ï¼š
        
        URL: {url}
        é“¾æ¥æ–‡æœ¬: {link_text}
        URLè·¯å¾„: {url_path}
        
        è¯·åˆ¤æ–­æ­¤é“¾æ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ–°é—»é“¾æ¥ï¼Œå¹¶ç»™å‡º0-100çš„åˆ†æ•°ã€‚
        æœ‰æ•ˆæ–°é—»é“¾æ¥ç‰¹å¾:
        - é“¾æ¥æŒ‡å‘å…·ä½“æ–°é—»æ–‡ç« è€Œéä¸»é¡µæˆ–æ ç›®é¡µ
        - URLç»“æ„é€šå¸¸åŒ…å«æ—¥æœŸæˆ–æ–‡ç« æ ‡è¯†ç¬¦
        - é“¾æ¥æ–‡æœ¬çœ‹èµ·æ¥åƒæ–°é—»æ ‡é¢˜
        - é€šå¸¸ä¸åŒ…å«tagã€loginã€searchç­‰éæ–‡ç« è·¯å¾„
        
        è¯·ç›´æ¥è¿”å›ä»¥ä¸‹JSONæ ¼å¼ç»“æœ:
        {{"score": åˆ†æ•°å€¼(0-100), "is_valid": true/false, "reason": "ç®€è¦åˆ†æåŸå› "}}
        """
        
        BATCH_LINK_VALIDATION_PROMPT = """
        è¯·åˆ¤æ–­ä»¥ä¸‹é“¾æ¥åˆ—è¡¨ä¸­å“ªäº›æ˜¯æœ‰æ•ˆçš„å†œä¸šæ–°é—»é“¾æ¥:
        
        {links_json}
        
        å¯¹æ¯ä¸ªé“¾æ¥è¯„ä¼°ï¼Œç»™å‡º0-100åˆ†æ•°ï¼Œå¹¶æ ‡è®°æ˜¯å¦æœ‰æ•ˆã€‚
        æœ‰æ•ˆæ–°é—»é“¾æ¥:
        - æŒ‡å‘å…·ä½“æ–‡ç« é¡µé¢
        - é€šå¸¸æœ‰æ–‡ç« æ ‡è¯†ç¬¦æˆ–æ—¥æœŸ
        - ä¸æ˜¯tagã€ç±»åˆ«ã€æœç´¢é¡µé¢
        
        ç›´æ¥è¿”å›JSONåˆ—è¡¨:
        [
          {{"url": "é“¾æ¥1", "score": åˆ†æ•°å€¼, "is_valid": true/false}},
          {{"url": "é“¾æ¥2", "score": åˆ†æ•°å€¼, "is_valid": true/false}},
          ...
        ]
        """

# é…ç½®DeepSeek API
openai.api_key = "sk-86b89a0e6b024d03a2421cf5bf7e2d82"
openai.base_url = "https://api.deepseek.com/v1"

# åˆ›å»ºå®¢æˆ·ç«¯
client = openai.OpenAI(
    api_key = "sk-86b89a0e6b024d03a2421cf5bf7e2d82",
    base_url = "https://api.deepseek.com/v1"
)

# é…ç½®é€‰é¡¹
ENABLE_LOGGING = True                # æ˜¯å¦è®°å½•AIåˆ¤æ–­ç»“æœ
LOG_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "ai_link_decisions.jsonl") # æ—¥å¿—æ–‡ä»¶è·¯å¾„
VALID_SCORE_THRESHOLD = 60           # æœ‰æ•ˆé“¾æ¥åˆ†æ•°é˜ˆå€¼
MAX_BATCH_SIZE = 5                   # æ‰¹é‡éªŒè¯çš„æœ€å¤§é“¾æ¥æ•°

# é“¾æ¥åˆ¤æ–­ç»“æœç¼“å­˜
url_judgment_cache = {}

def extract_basic_features(url, a_tag=None):
    """
    ä»URLå’Œé“¾æ¥æ ‡ç­¾ä¸­æå–æœ€åŸºæœ¬çš„ç‰¹å¾
    
    å‚æ•°:
        url: å¾…æ£€æŸ¥çš„URL
        a_tag: å¯é€‰çš„é“¾æ¥æ ‡ç­¾å¯¹è±¡
    
    è¿”å›:
        åŒ…å«æå–ç‰¹å¾çš„å­—å…¸
    """
    # è§£æURL
    parsed = urlparse(url)
    
    # æå–åŸºæœ¬ç‰¹å¾
    features = {
        "url": url,
        "url_path": parsed.path,
        "link_text": a_tag.get_text().strip() if a_tag else ""
    }
    
    return features

def log_link_decision(url, is_valid, score, reason=""):
    """è®°å½•é“¾æ¥åˆ¤æ–­ç»“æœ"""
    if not ENABLE_LOGGING:
        return
        
    try:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            decision = {
                "url": url,
                "is_valid": is_valid,
                "score": score,
                "reason": reason,
                "timestamp": datetime.now().isoformat()
            }
            f.write(json.dumps(decision, ensure_ascii=False) + "\n")
    except Exception as e:
        print(f"è®°å½•é“¾æ¥åˆ¤æ–­ç»“æœæ—¶å‡ºé”™: {e}")

def is_valid_news_link_with_ai(link, base_url, a_tag=None, html_content=None):
    """
    ä½¿ç”¨DeepSeek AIåˆ¤æ–­é“¾æ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ–°é—»é“¾æ¥
    
    å‚æ•°:
        link: å¾…æ£€æŸ¥çš„é“¾æ¥
        base_url: åŸºç¡€URL
        a_tag: å¯é€‰çš„é“¾æ¥æ ‡ç­¾
        html_content: å¯é€‰çš„HTMLå†…å®¹ï¼ˆæ­¤ç‰ˆæœ¬æœªä½¿ç”¨ï¼‰
    
    è¿”å›:
        å¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºé“¾æ¥æ˜¯å¦æœ‰æ•ˆ
    """
    try:
        # åŸºç¡€è¿‡æ»¤ - æ˜æ˜¾çš„éæ–°é—»é“¾æ¥
        if not link or not isinstance(link, str):
            print(f"  âŒ è·³è¿‡æ— æ•ˆé“¾æ¥æ ¼å¼: {link}")
            return False
            
        if link.startswith(('#', 'javascript:', 'mailto:', 'tel:')):
            print(f"  âŒ è·³è¿‡ç‰¹æ®Šåè®®é“¾æ¥: {link}")
            return False
            
        # æ„å»ºå®Œæ•´URL
        if not urlparse(link).netloc:
            full_url = urljoin(base_url, link)
        else:
            full_url = link
            
        # æ£€æŸ¥ç¼“å­˜
        if full_url in url_judgment_cache:
            is_valid = url_judgment_cache[full_url]
            print(f"  {'âœ…' if is_valid else 'âŒ'} ç¼“å­˜ç»“æœ: {full_url} {'æœ‰æ•ˆ' if is_valid else 'æ— æ•ˆ'}")
            return is_valid
            
        # æ’é™¤åª’ä½“æ–‡ä»¶
        if any(urlparse(full_url).path.lower().endswith(ext) for ext in 
              ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.zip', '.doc', '.css', '.js']):
            print(f"  âŒ è·³è¿‡åª’ä½“/èµ„æºæ–‡ä»¶: {full_url}")
            url_judgment_cache[full_url] = False
            return False
        
        # æå–ç‰¹å¾
        print(f"  ğŸ” æå–é“¾æ¥ç‰¹å¾: {full_url}")
        features = extract_basic_features(full_url, a_tag)
        link_text = features["link_text"]
        if link_text:
            print(f"  ğŸ“ é“¾æ¥æ–‡æœ¬: \"{link_text[:50]}{'...' if len(link_text) > 50 else ''}\"")
        
        # ä½¿ç”¨promptæ¨¡æ¿
        prompt = LINK_VALIDATION_PROMPT.format(
            url=features["url"],
            link_text=features["link_text"],
            url_path=features["url_path"]
        )
        
        try:
            # è°ƒç”¨DeepSeek API
            print(f"  ğŸ“¤ å‘é€éªŒè¯è¯·æ±‚åˆ°DeepSeek API...")
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}]
            )
            
            # è§£æç»“æœ
            print(f"  ğŸ“¥ æ”¶åˆ°DeepSeek APIå“åº”ï¼Œè§£æä¸­...")
            answer = response.choices[0].message.content
            
            try:
                # å°è¯•è§£æJSONå“åº”
                # æ¸…ç†å¯èƒ½çš„Markdownä»£ç å—è¯­æ³•
                cleaned_answer = answer
                if "```json" in answer:
                    # æå–ä»£ç å—å†…å®¹
                    cleaned_answer = re.search(r'```json\s*(.*?)\s*```', answer, re.DOTALL)
                    if cleaned_answer:
                        cleaned_answer = cleaned_answer.group(1).strip()
                    else:
                        # å¦‚æœæ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•ç®€å•åˆ é™¤æ ‡è®°
                        cleaned_answer = answer.replace("```json", "").replace("```", "").strip()
                
                # å°è¯•è§£æJSON
                result = json.loads(cleaned_answer)
                
                score = result.get("score", 0)
                is_valid = result.get("is_valid", False)
                reason = result.get("reason", "")
                
                # è®°å½•åˆ¤æ–­ç»“æœ
                log_link_decision(full_url, is_valid, score, reason)
                
                # ç¼“å­˜ç»“æœ
                url_judgment_cache[full_url] = is_valid
                
                print(f"  {'âœ…' if is_valid else 'âŒ'} AIåˆ¤æ–­ç»“æœ: {full_url} {'æœ‰æ•ˆ' if is_valid else 'æ— æ•ˆ'} (åˆ†æ•°: {score})")
                if reason:
                    print(f"  ğŸ’¡ åˆ¤æ–­ç†ç”±: {reason}")
                
                return is_valid
                
            except json.JSONDecodeError:
                print(f"  âš ï¸ æ— æ³•è§£æAIå“åº”ä¸ºJSONï¼Œå°è¯•ç›´æ¥æå–åˆ†æ•°...")
                print(f"  ğŸ” åŸå§‹å“åº”: {answer[:100]}...")
                # å°è¯•ä»æ–‡æœ¬ä¸­æå–åˆ†æ•°
                score_match = re.search(r'score"?\s*:\s*(\d+)', answer)
                if score_match:
                    score = int(score_match.group(1))
                    is_valid = score >= VALID_SCORE_THRESHOLD
                    url_judgment_cache[full_url] = is_valid
                    print(f"  {'âœ…' if is_valid else 'âŒ'} ä»æ–‡æœ¬æå–çš„åˆ†æ•°: {score}ï¼Œç»“æœ: {'æœ‰æ•ˆ' if is_valid else 'æ— æ•ˆ'}")
                    return is_valid
                
                # å¦‚æœæ— æ³•æå–ï¼Œé»˜è®¤è¿”å›False
                print(f"  âŒ æ— æ³•ä»å“åº”ä¸­æå–ç»“æœï¼Œé»˜è®¤ä¸ºæ— æ•ˆ")
                url_judgment_cache[full_url] = False
                return False
                
        except Exception as e:
            print(f"  âš ï¸ è°ƒç”¨DeepSeek APIæ—¶å‡ºé”™: {str(e)}")
            # ç›´æ¥è¿”å›ç»“æœ
            url_judgment_cache[full_url] = False
            return False
            
    except Exception as e:
        print(f"  âš ï¸ AIåˆ¤æ–­é“¾æ¥æ—¶å‡ºé”™: {str(e)}, é“¾æ¥: {link}")
        return False

def batch_link_validation(links_info, base_url, html_content=None):
    """
    æ‰¹é‡éªŒè¯é“¾æ¥ï¼Œä¼˜åŒ–APIè°ƒç”¨é¢‘ç‡
    
    å‚æ•°:
        links_info: å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å« 'url', 'a_tag', 'title' é”®
        base_url: åŸºç¡€URL
        html_content: å¯é€‰çš„HTMLå†…å®¹
    
    è¿”å›:
        åˆ—è¡¨ï¼ŒåŒ…å«åˆ¤å®šä¸ºæœ‰æ•ˆçš„é“¾æ¥ä¿¡æ¯
    """
    valid_links = []
    
    # é¢„ç­›é€‰ - åŸºç¡€è§„åˆ™è¿‡æ»¤æ˜æ˜¾æ— æ•ˆçš„é“¾æ¥
    filtered_links = []
    
    print(f"ğŸ” å¼€å§‹ç­›é€‰ {len(links_info)} ä¸ªé“¾æ¥...")
    for link_info in links_info:
        # ç»Ÿä¸€å¤„ç†urlé”®
        url = None
        if 'url' in link_info:
            url = link_info['url']
        elif 'link' in link_info:
            url = link_info['link']
            # ç»Ÿä¸€ä¸ºurlé”®
            link_info['url'] = url
        
        # è·³è¿‡ç©ºé“¾æ¥æˆ–éå­—ç¬¦ä¸²é“¾æ¥
        if not url or not isinstance(url, str):
            continue
            
        # è·³è¿‡éHTTPé“¾æ¥
        if url.startswith(('#', 'javascript:', 'mailto:', 'tel:')):
            continue
            
        # æ„å»ºå®Œæ•´URL
        if not urlparse(url).netloc:
            full_url = urljoin(base_url, url)
        else:
            full_url = url
            
        # æ›´æ–°ä¸ºå®Œæ•´URL
        link_info['url'] = full_url
            
        # æ£€æŸ¥ç¼“å­˜
        if full_url in url_judgment_cache:
            if url_judgment_cache[full_url]:
                link_info['is_valid'] = True
                link_info['ai_score'] = 80  # é»˜è®¤åˆ†æ•°
                link_info['ai_reason'] = "ç¼“å­˜éªŒè¯é€šè¿‡"
                valid_links.append(link_info)
            continue
            
        # æ’é™¤åª’ä½“æ–‡ä»¶
        if any(urlparse(full_url).path.lower().endswith(ext) for ext in 
              ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.zip', '.doc', '.css', '.js']):
            url_judgment_cache[full_url] = False
            continue
            
        # æ·»åŠ åˆ°å¾…å¤„ç†åˆ—è¡¨
        filtered_links.append(link_info)
    
    # å¦‚æœæ²¡æœ‰é“¾æ¥éœ€è¦éªŒè¯ï¼Œç›´æ¥è¿”å›
    if not filtered_links:
        print("âš ï¸ åŸºç¡€ç­›é€‰åæ²¡æœ‰é“¾æ¥éœ€è¦AIéªŒè¯")
        # å…³é”®ä¿®æ”¹ï¼šå¦‚æœæ²¡æœ‰é“¾æ¥éœ€è¦éªŒè¯ï¼Œè¿”å›å·²éªŒè¯é€šè¿‡çš„é“¾æ¥
        return valid_links
    
    print(f"âœ… åŸºç¡€ç­›é€‰åæœ‰ {len(filtered_links)} ä¸ªé“¾æ¥éœ€è¦AIéªŒè¯")
    
    # å¤„ç†æœªç¼“å­˜çš„é“¾æ¥
    # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤šMAX_BATCH_SIZEä¸ªé“¾æ¥
    batch_count = (len(filtered_links) + MAX_BATCH_SIZE - 1) // MAX_BATCH_SIZE  # å‘ä¸Šå–æ•´è®¡ç®—æ‰¹æ¬¡æ•°
    print(f"ğŸ”„ å°†åˆ† {batch_count} æ‰¹è¿›è¡ŒAIéªŒè¯ï¼Œæ¯æ‰¹æœ€å¤š {MAX_BATCH_SIZE} ä¸ªé“¾æ¥")
    
    for i in range(0, len(filtered_links), MAX_BATCH_SIZE):
        batch = filtered_links[i:i+MAX_BATCH_SIZE]
        current_batch = i // MAX_BATCH_SIZE + 1
        print(f"ğŸ§  æ­£åœ¨éªŒè¯ç¬¬ {current_batch}/{batch_count} æ‰¹ï¼ŒåŒ…å« {len(batch)} ä¸ªé“¾æ¥...")
        
        if len(batch) == 1:
            # å•é“¾æ¥å¤„ç†
            link_info = batch[0]
            print(f"  - éªŒè¯å•ä¸ªé“¾æ¥: {link_info['url']}")
            if is_valid_news_link_with_ai(link_info['url'], base_url, link_info.get('a_tag')):
                # æ·»åŠ éªŒè¯ä¿¡æ¯
                link_info['is_valid'] = True
                link_info['ai_score'] = 80  # é»˜è®¤åˆ†æ•°
                link_info['ai_reason'] = "AIéªŒè¯é€šè¿‡"
                valid_links.append(link_info)
                print(f"  âœ… é“¾æ¥æœ‰æ•ˆ: {link_info['url']}")
            else:
                print(f"  âŒ é“¾æ¥æ— æ•ˆ: {link_info['url']}")
        else:
            # æ‰¹é‡å¤„ç†
            try:
                # å‡†å¤‡æ‰¹é‡å¤„ç†çš„é“¾æ¥æ•°æ®
                links_data = []
                for link_info in batch:
                    links_data.append({
                        "url": link_info['url'],
                        "text": link_info.get('title', '') or (link_info.get('a_tag').get_text().strip() if link_info.get('a_tag') else '')
                    })
                
                # æ„å»ºæ‰¹é‡éªŒè¯prompt
                prompt = BATCH_LINK_VALIDATION_PROMPT.format(
                    links_json=json.dumps(links_data, ensure_ascii=False)
                )
                
                print(f"  ğŸ“¤ å‘é€æ‰¹é‡éªŒè¯è¯·æ±‚åˆ°DeepSeek API...")
                # è°ƒç”¨DeepSeek APIï¼ˆæ›´æ–°ä¸ºæ–°ç‰ˆæœ¬APIï¼‰
                response = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[{"role": "user", "content": prompt}]
                )
                
                print(f"  ğŸ“¥ æ”¶åˆ°DeepSeek APIå“åº”ï¼Œæ­£åœ¨è§£æ...")
                # è§£æç»“æœ (æ›´æ–°ä¸ºæ–°ç‰ˆæœ¬APIçš„å“åº”æ ¼å¼)
                answer = response.choices[0].message.content
                
                try:
                    # å°è¯•è§£æJSONå“åº” (å¢å¼ºå¤„ç†Markdownä»£ç å—çš„èƒ½åŠ›)
                    # æ¸…ç†å¯èƒ½çš„Markdownä»£ç å—è¯­æ³•
                    cleaned_answer = answer
                    if "```json" in answer:
                        # æå–ä»£ç å—å†…å®¹
                        cleaned_answer = re.search(r'```json\s*(.*?)\s*```', answer, re.DOTALL)
                        if cleaned_answer:
                            cleaned_answer = cleaned_answer.group(1).strip()
                        else:
                            # å¦‚æœæ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•ç®€å•åˆ é™¤æ ‡è®°
                            cleaned_answer = answer.replace("```json", "").replace("```", "").strip()
                    
                    # å°è¯•è§£æJSON
                    result = json.loads(cleaned_answer)
                    
                    # å¤„ç†æ‰¹é‡ç»“æœ
                    valid_count = 0
                    for item in result:
                        url = item.get("url")
                        is_valid = item.get("is_valid", False)
                        score = item.get("score", 0)
                        reason = item.get("reason", "")
                        
                        # ç¼“å­˜ç»“æœ
                        url_judgment_cache[url] = is_valid
                        
                        # å¦‚æœæœ‰æ•ˆï¼Œæ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                        if is_valid:
                            valid_count += 1
                            # æ‰¾åˆ°å¯¹åº”çš„link_info
                            for link_info in batch:
                                if link_info['url'] == url:
                                    # æ·»åŠ éªŒè¯ä¿¡æ¯
                                    link_info['is_valid'] = True
                                    link_info['ai_score'] = score
                                    link_info['ai_reason'] = reason or "AIéªŒè¯é€šè¿‡"
                                    valid_links.append(link_info)
                                    print(f"  âœ… é“¾æ¥æœ‰æ•ˆ (åˆ†æ•°: {score}): {url}")
                                    break
                        else:
                            print(f"  âŒ é“¾æ¥æ— æ•ˆ (åˆ†æ•°: {score}): {url}")
                        
                        # è®°å½•æ—¥å¿—
                        log_link_decision(url, is_valid, score, reason)
                    
                    print(f"  ğŸ“Š æ‰¹æ¬¡ç»“æœ: {valid_count}/{len(batch)} ä¸ªæœ‰æ•ˆé“¾æ¥")
                        
                except json.JSONDecodeError:
                    print(f"  âš ï¸ æ— æ³•è§£ææ‰¹é‡éªŒè¯å“åº”ä¸ºJSON: {answer}")
                    print(f"  ğŸ”„ å›é€€åˆ°é€ä¸ªéªŒè¯...")
                    # å›é€€åˆ°é€ä¸ªéªŒè¯
                    for idx, link_info in enumerate(batch):
                        print(f"    ğŸ” éªŒè¯é“¾æ¥ {idx+1}/{len(batch)}: {link_info['url']}")
                        is_valid = is_valid_news_link_with_ai(link_info['url'], base_url, link_info.get('a_tag'))
                        if is_valid:
                            # æ·»åŠ éªŒè¯ä¿¡æ¯
                            link_info['is_valid'] = True
                            link_info['ai_score'] = 80  # é»˜è®¤åˆ†æ•°
                            link_info['ai_reason'] = "å•ç‹¬AIéªŒè¯é€šè¿‡"
                            valid_links.append(link_info)
                            print(f"    âœ… é“¾æ¥æœ‰æ•ˆ: {link_info['url']}")
                        else:
                            print(f"    âŒ é“¾æ¥æ— æ•ˆ: {link_info['url']}")
                            
            except Exception as e:
                print(f"  âš ï¸ æ‰¹é‡éªŒè¯å‡ºé”™: {e}")
                print(f"  ğŸ”„ å›é€€åˆ°é€ä¸ªéªŒè¯...")
                # å›é€€åˆ°é€ä¸ªéªŒè¯
                for idx, link_info in enumerate(batch):
                    print(f"    ğŸ” éªŒè¯é“¾æ¥ {idx+1}/{len(batch)}: {link_info['url']}")
                    is_valid = is_valid_news_link_with_ai(link_info['url'], base_url, link_info.get('a_tag'))
                    if is_valid:
                        # æ·»åŠ éªŒè¯ä¿¡æ¯
                        link_info['is_valid'] = True
                        link_info['ai_score'] = 80  # é»˜è®¤åˆ†æ•°
                        link_info['ai_reason'] = "å•ç‹¬AIéªŒè¯é€šè¿‡"
                        valid_links.append(link_info)
                        print(f"    âœ… é“¾æ¥æœ‰æ•ˆ: {link_info['url']}")
                    else:
                        print(f"    âŒ é“¾æ¥æ— æ•ˆ: {link_info['url']}")
    
    print(f"ğŸ AIéªŒè¯å®Œæˆï¼Œæ€»å…±æœ‰ {len(valid_links)}/{len(links_info)} ä¸ªæœ‰æ•ˆé“¾æ¥")
    return valid_links

# å¯¼å‡ºå‡½æ•°
__all__ = ['is_valid_news_link_with_ai', 'batch_link_validation'] 