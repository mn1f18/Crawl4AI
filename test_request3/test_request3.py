import openpyxl
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode
from bs4 import BeautifulSoup
import os
import json
import re
import traceback
from datetime import datetime
from urllib.parse import urlparse, urljoin, urlunparse
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawl4ai import CacheMode
import requests
import pandas as pd
import time
import hashlib
from concurrent.futures import ThreadPoolExecutor
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter

# å¯¼å…¥AIé“¾æ¥éªŒè¯æ¨¡å—
try:
    # å°è¯•ç›´æ¥å¯¼å…¥ï¼ˆå½“å‰ç›®å½•ä¸‹çš„æ¨¡å—ï¼‰
    import ai_link_validator
    AI_LINK_VALIDATOR_AVAILABLE = True
except ImportError:
    try:
        # å°è¯•ä»ä¸Šçº§ç›®å½•å¯¼å…¥
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        import ai_link_validator
        AI_LINK_VALIDATOR_AVAILABLE = True
    except ImportError:
        print("âš ï¸ AIé“¾æ¥éªŒè¯æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•åˆ¤æ–­é“¾æ¥æœ‰æ•ˆæ€§")
        AI_LINK_VALIDATOR_AVAILABLE = False

# é…ç½®é€‰é¡¹
SAVE_TO_EXCEL = True                # æ˜¯å¦ä¿å­˜ç»“æœåˆ°Excel
MAX_LINKS_PER_SOURCE = 100           # æ¯ä¸ªæ¥æºæœ€å¤šæŠ“å–çš„é“¾æ¥æ•°
MIN_CONTENT_LENGTH = 300            # æœ€å°æœ‰æ•ˆå†…å®¹é•¿åº¦
MAX_RETRY_COUNT = 2                 # é“¾æ¥è¯·æ±‚å¤±è´¥æ—¶æœ€å¤§é‡è¯•æ¬¡æ•°
REQUEST_TIMEOUT = 60                # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
SKIP_EXISTING_LINKS = True          # æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„é“¾æ¥
LINKS_HISTORY_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "link_history")  # å†å²é“¾æ¥å­˜å‚¨ç›®å½•
USE_AI_LINK_VALIDATION = True       # æ˜¯å¦ä½¿ç”¨AIé“¾æ¥éªŒè¯
USE_COLD_START = False             # æ˜¯å¦ä½¿ç”¨å†·å¯åŠ¨æ¨¡å¼ï¼ˆè·³è¿‡AIéªŒè¯ï¼Œç›´æ¥çˆ¬å–å†…å®¹ï¼‰

# ç®€åŒ–è¯„åˆ†å‚æ•° - ä»…ä¿ç•™åŸºç¡€è®¾ç½®
QUALITY_CONFIG = {
    "min_content_length": 300,    # æœ€å°æœ‰æ•ˆå†…å®¹é•¿åº¦
    "min_paragraphs": 3           # æœ€å°‘æ®µè½æ•°
}

# ç¡®ä¿å†å²ç›®å½•å­˜åœ¨
if not os.path.exists(LINKS_HISTORY_DIR):
    os.makedirs(LINKS_HISTORY_DIR)

# ä½¿ç”¨URLä½œä¸ºé“¾æ¥å”¯ä¸€æ ‡è¯†
def generate_link_id(url):
    """ç”Ÿæˆé“¾æ¥çš„å”¯ä¸€æ ‡è¯†ï¼Œç›´æ¥ä½¿ç”¨URLä½œä¸ºæ ‡è¯†"""
    return url

# æ·»åŠ URLè§„èŒƒåŒ–å‡½æ•°
def normalize_url(url):
    """
    è§„èŒƒåŒ–URLï¼Œç”¨äºé“¾æ¥å»é‡
    - è½¬æ¢ä¸ºå°å†™
    - ç§»é™¤URLå‚æ•°ï¼ˆå¦‚æœä¸åŒ…å«ç‰¹å®šå…³é”®è¯ï¼‰
    - å»é™¤å°¾éƒ¨æ–œæ 
    - ç§»é™¤é»˜è®¤ç«¯å£å·
    """
    if not url:
        return ""
        
    parsed = urlparse(url)
    
    # å°å†™å¤„ç†åŸŸå
    netloc = parsed.netloc.lower()
    
    # ç§»é™¤é»˜è®¤ç«¯å£å·
    if netloc.endswith(':80') and parsed.scheme == 'http':
        netloc = netloc[:-3]
    elif netloc.endswith(':443') and parsed.scheme == 'https':
        netloc = netloc[:-4]
    
    # å¤„ç†è·¯å¾„ - å°å†™å¹¶ç§»é™¤å°¾éƒ¨æ–œæ 
    path = parsed.path.lower()
    if path.endswith('/') and len(path) > 1:
        path = path[:-1]
    
    # æ£€æŸ¥æ˜¯å¦ä¿ç•™æŸ¥è¯¢å‚æ•° - æŸäº›ç½‘ç«™ä½¿ç”¨æŸ¥è¯¢å‚æ•°åŒºåˆ†æ–‡ç« 
    query = parsed.query
    
    # ä¿ç•™å«æœ‰è¿™äº›è¯çš„æŸ¥è¯¢å‚æ•°ï¼Œå®ƒä»¬é€šå¸¸ç”¨äºåŒºåˆ†å†…å®¹
    keep_query_keywords = ['id', 'article', 'news', 'post', 'story', 'p']
    
    # å¦‚æœæŸ¥è¯¢å‚æ•°ä¸åŒ…å«ä¿ç•™å…³é”®è¯ï¼Œç§»é™¤å®ƒä»¬
    if query and not any(keyword in query.lower() for keyword in keep_query_keywords):
        query = ''
    
    # é‡å»ºURL
    normalized = urlunparse((
        parsed.scheme.lower(),
        netloc,
        path,
        parsed.params,
        query,
        '' # ç§»é™¤fragment
    ))
    
    return normalized

# æ£€æµ‹æ˜¯å¦ä¸ºæ–°é“¾æ¥å¹¶è¿½è¸ªé“¾æ¥çŠ¶æ€
def is_new_link(url, source):
    """
    æ£€æŸ¥é“¾æ¥æ˜¯å¦ä¸ºæ–°é“¾æ¥ï¼ˆæœªçˆ¬å–è¿‡ï¼‰å¹¶è·å–çŠ¶æ€
    
    è¿”å›: (is_new, is_invalid, is_processed)
    is_new: å¸ƒå°”å€¼ï¼ŒTrueè¡¨ç¤ºé“¾æ¥æœªçˆ¬å–è¿‡
    is_invalid: å¸ƒå°”å€¼ï¼ŒTrueè¡¨ç¤ºé“¾æ¥ä¹‹å‰å·²è¢«æ ‡è®°ä¸ºæ— æ•ˆ
    is_processed: å¸ƒå°”å€¼ï¼ŒTrueè¡¨ç¤ºé“¾æ¥å·²ç»è¢«å¤„ç†è¿‡ï¼ˆæ— è®ºæœ‰æ•ˆæˆ–æ— æ•ˆï¼‰
    """
    links_history = load_links_history(source)
    
    # ç›´æ¥æ£€æŸ¥URLæ˜¯å¦å­˜åœ¨äºå†å²è®°å½•ä¸­
    for _, info in links_history.items():
        if info.get('url', '') == url:
            is_valid = info.get('is_valid', True)
            # é“¾æ¥å­˜åœ¨ä¸”å·²ç»æŠ“å–è¿‡å†…å®¹ï¼ˆå†…å®¹é•¿åº¦å¤§äº0æˆ–çˆ¬å–æ¬¡æ•°å¤§äº1ï¼‰
            has_content = info.get('content_length', 0) > 0 or info.get('crawl_count', 0) > 1
            return False, not is_valid, has_content
    
    # URLä¸å­˜åœ¨äºå†å²è®°å½•ä¸­ï¼Œåˆ™ä¸ºæ–°é“¾æ¥ä¸”æœªå¤„ç†è¿‡
    return True, False, False

# åŠ è½½å†å²é“¾æ¥æ•°æ®
def load_links_history(source):
    """åŠ è½½æŒ‡å®šæ¥æºçš„å†å²é“¾æ¥æ•°æ®"""
    history_file = os.path.join(LINKS_HISTORY_DIR, f"{source}_links.json")
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"åŠ è½½å†å²é“¾æ¥æ•°æ®å‡ºé”™: {e}")
            return {}
    return {}

# ä¿å­˜å†å²é“¾æ¥æ•°æ®
def save_links_history(source, links_data):
    """ä¿å­˜æŒ‡å®šæ¥æºçš„å†å²é“¾æ¥æ•°æ®"""
    history_file = os.path.join(LINKS_HISTORY_DIR, f"{source}_links.json")
    try:
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(links_data, f, ensure_ascii=False, indent=2)
        print(f"æˆåŠŸä¿å­˜å†å²é“¾æ¥æ•°æ®: {history_file}")
    except Exception as e:
        print(f"ä¿å­˜å†å²é“¾æ¥æ•°æ®å‡ºé”™: {e}")

# æ›´æ–°é“¾æ¥å†å²è®°å½•
def update_link_history(url, title, source, content_summary="", is_valid=True, quality_score=0, content_length=0, error_message="", content_fingerprint="", ai_score=0, ai_reason="", link_type=""):
    """
    æ›´æ–°é“¾æ¥å†å²è®°å½•
    
    å‚æ•°:
        url: é“¾æ¥URL
        title: é“¾æ¥æ ‡é¢˜
        source: æ¥æºID
        content_summary: å†…å®¹æ‘˜è¦
        is_valid: æ˜¯å¦æœ‰æ•ˆçš„é“¾æ¥
        quality_score: å†…å®¹è´¨é‡è¯„åˆ†
        content_length: å†…å®¹é•¿åº¦
        error_message: é”™è¯¯ä¿¡æ¯
        content_fingerprint: å†…å®¹æŒ‡çº¹
        ai_score: AIéªŒè¯åˆ†æ•°
        ai_reason: AIéªŒè¯ç†ç”±
        link_type: é“¾æ¥ç±»å‹
    """
    try:
        # åŠ è½½å†å²è®°å½•
        links_history = load_links_history(source)
        
        # ç”Ÿæˆé“¾æ¥ID
        link_id = generate_link_id(url)
        
        # å½“å‰æ—¶é—´
        current_time = datetime.now().isoformat()
        
        # æ£€æŸ¥æ­¤é“¾æ¥æ˜¯å¦å­˜åœ¨
        if link_id in links_history:
            # æ›´æ–°ç°æœ‰è®°å½•
            links_history[link_id]['is_valid'] = is_valid
            links_history[link_id]['last_updated'] = current_time
            
            # åªæœ‰åœ¨æä¾›äº†å†…å®¹æ—¶æ‰æ›´æ–°å†…å®¹ç›¸å…³å­—æ®µ
            if content_length > 0:
                links_history[link_id]['content_length'] = content_length
                if content_summary:
                    links_history[link_id]['content_summary'] = content_summary
                if content_fingerprint:
                    links_history[link_id]['content_fingerprint'] = content_fingerprint
            
            # æ›´æ–°è´¨é‡è¯„åˆ†ï¼ˆå¦‚æœæä¾›ï¼‰
            if quality_score > 0:
                links_history[link_id]['quality_score'] = quality_score
                
            # æ›´æ–°é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if error_message:
                links_history[link_id]['error_message'] = error_message
                
            # å¢åŠ çˆ¬å–æ¬¡æ•°
            links_history[link_id]['crawl_count'] = links_history[link_id].get('crawl_count', 0) + 1
                
            # æ›´æ–°AIéªŒè¯ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
            if ai_score > 0:
                links_history[link_id]['ai_score'] = ai_score
            if ai_reason:
                links_history[link_id]['ai_reason'] = ai_reason
            if link_type:
                links_history[link_id]['link_type'] = link_type
        else:
            # åˆ›å»ºæ–°è®°å½•
            links_history[link_id] = {
                'url': url,
                'title': title,
                'is_valid': is_valid,
                'first_seen': current_time,
                'last_updated': current_time,
                'content_length': content_length,
                'content_summary': content_summary,
                'quality_score': quality_score,
                'error_message': error_message,
                'content_fingerprint': content_fingerprint,
                'crawl_count': 1,  # åˆå§‹çˆ¬å–æ¬¡æ•°ä¸º1
                'ai_score': ai_score,
                'ai_reason': ai_reason,
                'link_type': link_type
            }
            
        # ä¿å­˜æ›´æ–°åçš„å†å²è®°å½•
        save_links_history(source, links_history)
        
    except Exception as e:
        print(f"æ›´æ–°é“¾æ¥å†å²è®°å½•å‡ºé”™: {e}")

# è¯»å–ä¸»ç•Œé¢é“¾æ¥
# ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé¿å…ç›¸å¯¹è·¯å¾„å¯¼è‡´çš„é—®é¢˜
excel_source_file = r'C:\Python\github\Crawl4AI\testhomepage.xlsx'  # ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²å’Œåæ–œæ 
# å¤‡é€‰ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœç»å¯¹è·¯å¾„ä¸å­˜åœ¨å¯ä»¥å°è¯•
alternative_path = '../testhomepage.xlsx'

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°è¯•å¤‡é€‰è·¯å¾„
if not os.path.exists(excel_source_file):
    print(f"âŒ ä¸»è·¯å¾„æ‰¾ä¸åˆ°æ–‡ä»¶: {excel_source_file}ï¼Œå°è¯•å¤‡é€‰è·¯å¾„...")
    if os.path.exists(alternative_path):
        excel_source_file = alternative_path
        print(f"âœ… æ‰¾åˆ°å¤‡é€‰è·¯å¾„æ–‡ä»¶: {excel_source_file}")
    else:
        print(f"âŒ å¤‡é€‰è·¯å¾„ä¹Ÿæ‰¾ä¸åˆ°æ–‡ä»¶: {alternative_path}")
        # å°è¯•åˆ—å‡ºå½“å‰ç›®å½•ä¸‹çš„Excelæ–‡ä»¶
        print("å½“å‰ç›®å½•ä¸‹çš„Excelæ–‡ä»¶:")
        for file in os.listdir('.'):
            if file.endswith('.xlsx'):
                print(f" - {file}")
        
        # å°è¯•åˆ—å‡ºä¸Šçº§ç›®å½•ä¸‹çš„Excelæ–‡ä»¶
        print("ä¸Šçº§ç›®å½•ä¸‹çš„Excelæ–‡ä»¶:")
        for file in os.listdir('..'):
            if file.endswith('.xlsx'):
                print(f" - {file}")
else:
    print(f"âœ… æ‰¾åˆ°æºæ–‡ä»¶: {excel_source_file}")

wb = openpyxl.load_workbook(excel_source_file)
sheet = wb.active

# åˆ›å»ºç»“æœExcelæ–‡ä»¶
def create_result_excel(filename=None):
    """åˆ›å»ºç»“æœExcelæ–‡ä»¶"""
    # å¦‚æœä¸ä¿å­˜åˆ°Excelï¼Œç›´æ¥è¿”å›None
    if not SAVE_TO_EXCEL:
        return None
    
    # ç”Ÿæˆæ–‡ä»¶å
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"news_results_{timestamp}.xlsx"
    
    # ä¿å­˜å…¨å±€å¼•ç”¨
    global result_excel_file
    result_excel_file = filename
    
    # åˆ›å»ºå·¥ä½œç°¿å’Œå·¥ä½œè¡¨
    wb = openpyxl.Workbook()
    sheet = wb.active
    sheet.title = "æŠ“å–ç»“æœ"
    
    # è®¾ç½®è¡¨å¤´
    headers = [
        'ç´¢å¼•', 'æ¥æº', 'é“¾æ¥', 'æ ‡é¢˜', 'å‘å¸ƒæ—¥æœŸ', 'åˆ†ç±»', 'æ­£æ–‡å­—æ•°', 
        'çˆ¬å–æ—¶é—´(ç§’)', 'çŠ¶æ€', 'å°è¯•æ¬¡æ•°', 'AIéªŒè¯åˆ†æ•°', 'AIéªŒè¯ç†ç”±', 'é“¾æ¥ç±»å‹'
    ]
    
    # è®¾ç½®åˆ—å®½
    column_widths = {
        'A': 8,   # ç´¢å¼•
        'B': 15,  # æ¥æº
        'C': 50,  # é“¾æ¥
        'D': 50,  # æ ‡é¢˜
        'E': 15,  # å‘å¸ƒæ—¥æœŸ
        'F': 15,  # åˆ†ç±»
        'G': 10,  # æ­£æ–‡å­—æ•°
        'H': 15,  # çˆ¬å–æ—¶é—´
        'I': 20,  # çŠ¶æ€
        'J': 10,  # å°è¯•æ¬¡æ•°
        'K': 12,  # AIéªŒè¯åˆ†æ•°
        'L': 40,  # AIéªŒè¯ç†ç”±
        'M': 15,  # é“¾æ¥ç±»å‹
    }
    
    for col, width in column_widths.items():
        sheet.column_dimensions[col].width = width
    
    # å†™å…¥è¡¨å¤´
    for idx, header in enumerate(headers, 1):
        sheet.cell(row=1, column=idx).value = header
    
    # ä¿å­˜å¹¶è¿”å›
    wb.save(filename)
    print(f"å·²åˆ›å»ºç»“æœæ–‡ä»¶: {filename}")
    return filename

# ç”Ÿæˆå½“å‰æ‰¹æ¬¡ID
current_batch = datetime.now().strftime("%Y%m%d_%H%M%S")
batch_id = current_batch

# å…¨å±€å˜é‡
result_excel_file = f"news_results_{batch_id}.xlsx"  # é»˜è®¤Excelæ–‡ä»¶å
LOG_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "ai_link_decisions.jsonl")  # AIéªŒè¯æ—¥å¿—æ–‡ä»¶è·¯å¾„
ENABLE_LOGGING = True  # æ˜¯å¦å¯ç”¨AIéªŒè¯æ—¥å¿—

# ç®€åŒ–è¯„ä¼°å†…å®¹è´¨é‡çš„å‡½æ•°
def evaluate_content_quality(html_content, title, url=""):
    """
    ç®€åŒ–ç‰ˆå‡½æ•°ï¼Œä¸å†è¯„ä¼°å†…å®¹è´¨é‡ï¼Œåªè¿”å›å†…å®¹æ‘˜è¦å’ŒæŒ‡çº¹
    """
    if not html_content:
        return True, "", ""
    
    # ä½¿ç”¨BeautifulSoupè§£æHTML
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ç§»é™¤è„šæœ¬ã€æ ·å¼å’Œå¯¼èˆªå…ƒç´ 
    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
        tag.decompose()
    
    # è·å–æ‰€æœ‰æ–‡æœ¬
    text = soup.get_text(separator='\n', strip=True)
    
    # ç”Ÿæˆå†…å®¹æ‘˜è¦ (é™åˆ¶ä¸º200å­—)
    content_summary = text[:200] + "..." if len(text) > 200 else text
    
    # ç”Ÿæˆå†…å®¹æŒ‡çº¹
    content_fingerprint = hashlib.md5(text[:1000].encode('utf-8')).hexdigest()
    
    # å§‹ç»ˆè¿”å›Trueè¡¨ç¤ºå†…å®¹æœ‰æ•ˆ
    return True, content_summary, content_fingerprint

# æå–å‘å¸ƒæ—¶é—´çš„å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰
def extract_publish_date(soup):
    """
    ä»HTMLä¸­æå–å‘å¸ƒæ—¶é—´ï¼Œå°è¯•å¤šç§æ–¹æ³•
    """
    # æ–¹æ³•1: æŸ¥æ‰¾å…ƒæ•°æ®æ ‡ç­¾
    meta_properties = [
        'article:published_time', 'datePublished', 'publishedDate', 
        'pubdate', 'date', 'DC.date', 'article:modified_time', 
        'lastModified', 'og:published_time', 'og:updated_time',
        'dateCreated', 'dateModified', 'release_date'
    ]
    
    for meta in soup.find_all('meta'):
        prop = meta.get('property', '') or meta.get('name', '')
        if prop.lower() in [p.lower() for p in meta_properties]:
            content = meta.get('content')
            if content:
                return content
    
    # æ–¹æ³•2: æŸ¥æ‰¾æ—¶é—´æ ‡ç­¾
    time_tags = soup.find_all('time')
    for time_tag in time_tags:
        datetime_attr = time_tag.get('datetime')
        if datetime_attr:
            return datetime_attr
        if time_tag.text.strip():
            return time_tag.text.strip()
    
    # æ–¹æ³•3: æ ¹æ®CSSé€‰æ‹©å™¨æŸ¥æ‰¾
    date_selectors = [
        '.date', '.time', '.published', '.timestamp', '.post-date',
        '[itemprop="datePublished"]', '.article-date', '.entry-date',
        '.post-meta time', '.article-meta time', '.publish-date',
        '.updated-date', '.create-date', '.release-date', '.article-time'
    ]
    
    for selector in date_selectors:
        date_element = soup.select_one(selector)
        if date_element:
            return re.sub(r'(Published|Updated|Posted|Date):?\s*', '', date_element.get_text(strip=True))
    
    # æ–¹æ³•4: åœ¨å…¨æ–‡ä¸­æŸ¥æ‰¾æ—¥æœŸæ¨¡å¼
    # å¸¸è§æ—¥æœŸæ ¼å¼ï¼šYYYY-MM-DD, DD/MM/YYYY, Month DD, YYYYç­‰
    date_patterns = [
        r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
        r'\d{2}/\d{2}/\d{4}',  # DD/MM/YYYY
        r'\d{2}\.\d{2}\.\d{4}', # DD.MM.YYYY
        r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s+\d{4}',
        r'\d{1,2}\s+(January|February|March|April|May|June|July|August|September|October|November|December),?\s+\d{4}'
    ]
    
    text = soup.get_text()
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    
    # æ²¡æœ‰æ‰¾åˆ°æ—¥æœŸ
    return "æœªçŸ¥"

# æå–æ–‡ç« æ ‡é¢˜çš„å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰
def extract_title(soup, url):
    """
    ä»HTMLä¸­æå–æ–‡ç« æ ‡é¢˜ï¼Œå°è¯•å¤šç§æ–¹æ³•
    """
    # æ–¹æ³•1: ä½¿ç”¨<title>æ ‡ç­¾
    if soup.title:
        title = soup.title.string
        # æ¸…ç†æ ‡é¢˜ï¼ˆç§»é™¤ç½‘ç«™åç§°ç­‰ï¼‰
        if title:
            # å¸¸è§åˆ†éš”ç¬¦ï¼š|, -, â€“, â€”, :, Â·, â€¢
            for separator in [' | ', ' - ', ' â€“ ', ' â€” ', ' : ', ' Â· ', ' â€¢ ']:
                if separator in title:
                    parts = title.split(separator)
                    # é€šå¸¸ç¬¬ä¸€éƒ¨åˆ†æ˜¯æ–‡ç« æ ‡é¢˜ï¼Œè€Œéç½‘ç«™åç§°
                    if len(parts[0]) > 10:  # æ ‡é¢˜è¶³å¤Ÿé•¿
                        return parts[0].strip()
            return title.strip()
    
    # æ–¹æ³•2: å¯»æ‰¾ä¸»æ ‡é¢˜æ ‡ç­¾
    for heading in ['h1', 'h2']:
        headings = soup.find_all(heading)
        if headings:
            # ç­›é€‰æœ€å¯èƒ½æ˜¯æ ‡é¢˜çš„å…ƒç´ ï¼ˆé€šå¸¸ä½äºå†…å®¹åŒºåŸŸï¼Œä¸åœ¨header/navç­‰ä¸­ï¼‰
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=lambda c: c and ('content' in c.lower() or 'article' in c.lower()))
            
            if main_content:
                content_headings = main_content.find_all(heading)
                if content_headings:
                    return content_headings[0].get_text(strip=True)
            
            # å¦‚æœä¸èƒ½ç¡®å®šå†…å®¹åŒºåŸŸï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæœ€é•¿çš„æ ‡é¢˜
            longest_heading = max(headings, key=lambda h: len(h.get_text(strip=True)))
            return longest_heading.get_text(strip=True)
    
    # æ–¹æ³•3: æŸ¥æ‰¾å…ƒæ•°æ®
    meta_tags = soup.find_all('meta')
    for meta in meta_tags:
        prop = meta.get('property', '') or meta.get('name', '')
        if prop.lower() in ['og:title', 'twitter:title', 'dc.title']:
            content = meta.get('content')
            if content:
                return content
    
    # æ–¹æ³•4: ä»URLä¸­æå–å¯èƒ½çš„æ ‡é¢˜
    path = urlparse(url).path
    segments = [seg for seg in path.split('/') if seg]
    if segments:
        # æœ€åä¸€æ®µè·¯å¾„å¯èƒ½æ˜¯æ ‡é¢˜çš„slug
        last_segment = segments[-1]
        # å°†è¿å­—ç¬¦æˆ–ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼
        title_from_url = re.sub(r'[-_]', ' ', last_segment)
        if len(title_from_url) > 5:  # ç¡®ä¿è¶³å¤Ÿé•¿
            return title_from_url.capitalize()
    
    # æ— æ³•ç¡®å®šæ ‡é¢˜
    return "æœªæ‰¾åˆ°æ ‡é¢˜"

# åˆ¤æ–­é“¾æ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ–°é—»é“¾æ¥çš„å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
def is_valid_news_link(link, base_url, a_tag=None, html_content=None):
    """
    åˆ¤æ–­é“¾æ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ–°é—»é“¾æ¥ - ç®€åŒ–ç‰ˆï¼Œä¸»è¦ä¾èµ–AI
    
    å‚æ•°:
        link: å¾…éªŒè¯çš„é“¾æ¥
        base_url: åŸºç¡€é“¾æ¥
        a_tag: å¯é€‰ï¼Œé“¾æ¥æ‰€åœ¨çš„aæ ‡ç­¾ï¼ˆBeautifulSoupå¯¹è±¡ï¼‰
        html_content: å¯é€‰ï¼Œé¡µé¢HTMLå†…å®¹
    """
    # åŸºç¡€è¿‡æ»¤ - æ˜æ˜¾çš„éæ–°é—»é“¾æ¥
    if not link or not isinstance(link, str):
        return False
        
    if link.startswith(('#', 'javascript:', 'mailto:', 'tel:')):
        return False
        
    # æ„å»ºå®Œæ•´URL
    if not urlparse(link).netloc:
        full_url = urljoin(base_url, link)
        parsed_link = urlparse(full_url)
    else:
        full_url = link
        parsed_link = urlparse(full_url)
        
    # æ’é™¤éæ–‡æœ¬åª’ä½“æ–‡ä»¶
    if any(parsed_link.path.lower().endswith(ext) for ext in 
          ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.zip', '.mp3', '.mp4', 
           '.css', '.js', '.ico', '.svg', '.webp', '.woff', '.ttf']):
        return False
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºåŒä¸€åŸŸåï¼ˆæ’é™¤å¤–éƒ¨é“¾æ¥ï¼‰
    parsed_base = urlparse(base_url)
    if parsed_link.netloc != parsed_base.netloc:
        return False

    # ä¼˜å…ˆä½¿ç”¨AIéªŒè¯æ¨¡å—
    if USE_AI_LINK_VALIDATION and AI_LINK_VALIDATOR_AVAILABLE:
        try:
            # ä½¿ç”¨AIéªŒè¯æ¨¡å—åˆ¤æ–­é“¾æ¥æœ‰æ•ˆæ€§
            return ai_link_validator.is_valid_news_link_with_ai(full_url, base_url, a_tag, html_content)
        except Exception as e:
            print(f"âŒ AIé“¾æ¥éªŒè¯å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€è§„åˆ™: {str(e)}")
    
    # åŸºç¡€è¿‡æ»¤è§„åˆ™ï¼ˆå¦‚æœAIéªŒè¯ä¸å¯ç”¨æˆ–å¤±è´¥ï¼‰
    # æ£€æŸ¥é“¾æ¥æ–‡æœ¬æ˜¯å¦çœ‹èµ·æ¥åƒæ ‡é¢˜ï¼ˆè¶…è¿‡20ä¸ªå­—ç¬¦ä¸”ä¸æ˜¯å¯¼èˆªç±»æ–‡æœ¬ï¼‰
    if a_tag and a_tag.get_text():
        link_text = a_tag.get_text().strip()
        if len(link_text) > 15:  # æ”¾å®½æ¡ä»¶
            return True
    
    # æ£€æŸ¥URLè·¯å¾„æ˜¯å¦åŒ…å«æ–°é—»ç›¸å…³ç‰¹å¾
    path_lower = parsed_link.path.lower()
    return (
        '/news/' in path_lower or 
        '/article/' in path_lower or 
        '/post/' in path_lower or
        re.search(r'/20\d{2}[/-]\d{1,2}/', path_lower) is not None or
        re.search(r'/\d{4,}/', path_lower) is not None
    )

# çˆ¬å–ä¸»ç•Œé¢
async def fetch_news_links(main_url, source):
    """ä»ä¸»é¡µè·å–æ–°é—»é“¾æ¥ï¼ŒæŒ‰æ–°é€»è¾‘å¤„ç†é“¾æ¥éªŒè¯å’Œçˆ¬å–"""
    browser_config = BrowserConfig(
        browser_type="chromium",
        headless=True, 
        viewport_width=1366,
        viewport_height=768,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
        java_script_enabled=True,
        ignore_https_errors=True
    )
    
    # è®¾ç½®çˆ¬è™«é…ç½®
    prune_filter = PruningContentFilter(
        threshold=0.45,
        threshold_type="dynamic",
        min_word_threshold=5
    )
    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)
    
    crawler_config = CrawlerRunConfig(
        markdown_generator=md_generator,
        page_timeout=REQUEST_TIMEOUT * 1000,
        cache_mode=CacheMode.BYPASS
    )
    
    try:
        print(f"ğŸ”„ æ­£åœ¨çˆ¬å–ä¸»é¡µ: {main_url}")
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=main_url, config=crawler_config)
            
            if not result.success:
                print(f"âŒ çˆ¬å–ä¸»é¡µå¤±è´¥: {main_url}, é”™è¯¯: {result.error_message}")
                return []
                
            print(f"âœ… çˆ¬å–ä¸»é¡µæˆåŠŸ: {main_url}")
            
            # åŠ è½½æ­¤æºç«™çš„å†å²é“¾æ¥æ•°æ®
            links_history = load_links_history(source)
            
            # è®°å½•å·²çŸ¥URLï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼ˆå†·å¯åŠ¨æ¨¡å¼å¯èƒ½ä¼šè·³è¿‡è¿™äº›æ£€æŸ¥ï¼‰
            known_urls = set()
            known_invalid_urls = set()  # è®°å½•å·²çŸ¥çš„æ— æ•ˆURL
            already_processed_urls = set()  # è®°å½•å·²å¤„ç†è¿‡çš„URLï¼ˆæ— è®ºæœ‰æ•ˆæ— æ•ˆï¼‰
            content_fingerprints = set()  # å­˜å‚¨æ‰€æœ‰å†…å®¹æŒ‡çº¹ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
            
            for _, info in links_history.items():
                url = info.get('url', '')
                known_urls.add(url)
                
                # è®°å½•å·²çŸ¥çš„æ— æ•ˆURL
                if not info.get('is_valid', True):
                    known_invalid_urls.add(url)
                
                # è®°å½•å·²ç»å¤„ç†è¿‡å†…å®¹çš„URL
                if info.get('content_length', 0) > 0 or info.get('crawl_count', 0) > 1:
                    already_processed_urls.add(url)
                
                # è®°å½•å†…å®¹æŒ‡çº¹ç”¨äºåç»­æ¯”è¾ƒ
                if info.get('content_fingerprint'):
                    content_fingerprints.add(info.get('content_fingerprint'))
                    
            print(f"âš ï¸ å·²åŠ è½½ {len(known_urls)} ä¸ªå·²çŸ¥é“¾æ¥ï¼Œå…¶ä¸­ {len(known_invalid_urls)} ä¸ªæ— æ•ˆé“¾æ¥")
            print(f"âš ï¸ å·²æœ‰ {len(already_processed_urls)} ä¸ªé“¾æ¥å·²ç»å¤„ç†è¿‡å†…å®¹")
            
            try:
                # ä½¿ç”¨BeautifulSoupå¤„ç†HTMLæå–é“¾æ¥
                soup = BeautifulSoup(result.html, 'html.parser')
                all_links = soup.find_all('a', href=True)
                print(f"ğŸ” æ‰¾åˆ° {len(all_links)} ä¸ªåŸå§‹é“¾æ¥")
                
                # ç”¨äºå­˜å‚¨å¤„ç†åçš„æ‰€æœ‰é“¾æ¥
                all_processed_links = []
                # ç”¨äºå­˜å‚¨æ–°é“¾æ¥ï¼ˆéœ€è¦éªŒè¯ï¼‰
                new_links_to_validate = []
                
                # æ·»åŠ ä¸€ä¸ªé›†åˆç”¨äºé“¾æ¥å»é‡
                processed_urls = set()
                skipped_duplicate_count = 0  # è®°å½•è·³è¿‡çš„é‡å¤é“¾æ¥æ•°
                
                # è·³è¿‡çš„é“¾æ¥è®¡æ•°
                skipped_known_valid_count = 0  # è·³è¿‡å·²çŸ¥æœ‰æ•ˆä¸”å·²å¤„ç†çš„é“¾æ¥
                skipped_invalid_count = 0  # è·³è¿‡å·²çŸ¥æ— æ•ˆé“¾æ¥
                skipped_already_processed = 0  # è·³è¿‡å·²ç»å¤„ç†è¿‡å†…å®¹çš„é“¾æ¥ï¼ˆæ— è®ºæœ‰æ•ˆæ— æ•ˆï¼‰
                
                for a_tag in all_links:
                    link = a_tag['href'].strip()
                    title = a_tag.get_text().strip()
                    
                    # è§„èŒƒåŒ–URL - å°†ç›¸å¯¹URLè½¬ä¸ºç»å¯¹URL
                    if link.startswith('/'):
                        # ç›¸å¯¹URLï¼Œæ·»åŠ åŸŸå
                        parsed_url = urlparse(main_url)
                        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
                        link = urljoin(base_url, link)
                    elif not link.startswith(('http://', 'https://')):
                        # å…¶ä»–ç›¸å¯¹URLå½¢å¼
                        link = urljoin(main_url, link)
                    
                    # è·³è¿‡éhttp(s)é“¾æ¥
                    if not link.startswith(('http://', 'https://')):
                        continue
                    
                    # è¿›ä¸€æ­¥è§„èŒƒåŒ–URLç”¨äºå»é‡
                    normalized_link = normalize_url(link)
                    
                    # é“¾æ¥å»é‡æ£€æŸ¥
                    if normalized_link in processed_urls:
                        skipped_duplicate_count += 1
                        continue
                    else:
                        processed_urls.add(normalized_link)
                    
                    # è·³è¿‡å·²çŸ¥é“¾æ¥ï¼ˆé™¤éæ˜¯å†·å¯åŠ¨æ¨¡å¼ï¼‰
                    if not USE_COLD_START:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºå·²çŸ¥é“¾æ¥ä»¥åŠçŠ¶æ€
                        is_new, is_invalid, is_processed = is_new_link(link, source)
                        
                        if not is_new:
                            if is_invalid:
                                # æ˜¯å·²çŸ¥çš„æ— æ•ˆé“¾æ¥ï¼Œç›´æ¥è·³è¿‡
                                skipped_invalid_count += 1
                                continue
                            elif is_processed:
                                # æ˜¯å·²ç»å¤„ç†è¿‡å†…å®¹çš„æœ‰æ•ˆé“¾æ¥ï¼Œç›´æ¥è·³è¿‡
                                skipped_already_processed += 1
                                continue
                            else:
                                # æ˜¯å·²çŸ¥çš„æœ‰æ•ˆé“¾æ¥ä½†æœªå¤„ç†è¿‡å†…å®¹ï¼Œå¯èƒ½éœ€è¦ç»§ç»­å¤„ç†
                                skipped_known_valid_count += 1
                                # æ ¹æ®éœ€è¦å†³å®šæ˜¯å¦è·³è¿‡
                                # ç”±äºéœ€è¦çˆ¬å–å†…å®¹ï¼Œè¿™é‡Œä¸è·³è¿‡ï¼Œè€Œæ˜¯åŠ å…¥å¾…éªŒè¯åˆ—è¡¨
                    else:
                        is_new = True
                        is_invalid = False
                        is_processed = False
                    
                    # ç”Ÿæˆé“¾æ¥ID
                    link_id = generate_link_id(link)
                    
                    # æ”¶é›†é“¾æ¥ä¿¡æ¯
                    link_info = {
                        'url': link,
                        'title': title,
                        'is_new': is_new,
                        'link_id': link_id,
                        'a_tag': a_tag,
                        'crawl_time': datetime.now().isoformat(),
                        'is_valid': False,  # é»˜è®¤ä¸ºæ— æ•ˆï¼Œç¨åéªŒè¯
                        'is_processed': is_processed,  # è®°å½•æ˜¯å¦å·²å¤„ç†è¿‡å†…å®¹
                    }
                    
                    all_processed_links.append(link_info)
                    
                    # æ ‡è®°ä¸ºéœ€è¦éªŒè¯çš„æ–°é“¾æ¥æˆ–æœªå¤„ç†è¿‡å†…å®¹çš„é“¾æ¥
                    if is_new or (not is_invalid and not is_processed):
                        new_links_to_validate.append(link_info)
                
                print(f"ğŸ”„ å¤„ç†åå¾—åˆ° {len(all_processed_links)} ä¸ªé“¾æ¥ï¼Œå…¶ä¸­ {len(new_links_to_validate)} ä¸ªéœ€è¦éªŒè¯")
                print(f"â© è·³è¿‡äº† {skipped_duplicate_count} ä¸ªé‡å¤é“¾æ¥")
                print(f"â© è·³è¿‡äº† {skipped_known_valid_count} ä¸ªå·²çŸ¥æœ‰æ•ˆé“¾æ¥")
                print(f"â© è·³è¿‡äº† {skipped_invalid_count} ä¸ªå·²çŸ¥æ— æ•ˆé“¾æ¥")
                print(f"â© è·³è¿‡äº† {skipped_already_processed} ä¸ªå·²å¤„ç†è¿‡å†…å®¹çš„é“¾æ¥")
                
                # è®¾ç½®valid_linksé»˜è®¤ä¸ºç©ºåˆ—è¡¨
                valid_links = []
                
                # å†·å¯åŠ¨æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰é“¾æ¥éƒ½è§†ä¸ºæœ‰æ•ˆï¼Œä¸è¿›è¡ŒAIéªŒè¯
                if USE_COLD_START:
                    print("ğŸš€ å†·å¯åŠ¨æ¨¡å¼: æ‰€æœ‰æ–°é“¾æ¥éƒ½å°†ç›´æ¥çˆ¬å–ï¼Œä¸éªŒè¯æœ‰æ•ˆæ€§")
                    # è®¾ç½®æ‰€æœ‰é“¾æ¥ä¸ºæœ‰æ•ˆ
                    for link in new_links_to_validate:
                        link['is_valid'] = True
                    valid_links = new_links_to_validate
                    
                    # æ›´æ–°æ‰€æœ‰é“¾æ¥çš„å†å²è®°å½•ï¼ˆæ ‡è®°ä¸ºæœ‰æ•ˆï¼‰
                    for link in valid_links:
                        update_link_history(
                            url=link['url'],
                            title=link['title'],
                            source=source,
                            is_valid=True,
                            quality_score=50  # å†·å¯åŠ¨æ¨¡å¼ä¸‹é»˜è®¤ä¸­ç­‰åˆ†æ•°
                        )
                
                # éå†·å¯åŠ¨æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨AIéªŒè¯æ–°é“¾æ¥
                elif USE_AI_LINK_VALIDATION and AI_LINK_VALIDATOR_AVAILABLE and new_links_to_validate:
                    print(f"ğŸ§  ä½¿ç”¨AIéªŒè¯ {len(new_links_to_validate)} ä¸ªé“¾æ¥...")
                    
                    # ä½¿ç”¨æ‰¹é‡éªŒè¯å¤„ç†æ–°é“¾æ¥
                    valid_links = ai_link_validator.batch_link_validation(
                        new_links_to_validate, 
                        main_url, 
                        result.html
                    )
                    
                    print(f"âœ… AIéªŒè¯å®Œæˆï¼Œæœ‰ {len(valid_links)} ä¸ªæœ‰æ•ˆé“¾æ¥")
                    
                    # æ›´æ–°æ‰€æœ‰ç»è¿‡éªŒè¯çš„é“¾æ¥å†å²è®°å½•
                    for link in new_links_to_validate:
                        # åˆ¤æ–­æ­¤é“¾æ¥æ˜¯å¦åœ¨æœ‰æ•ˆé“¾æ¥åˆ—è¡¨ä¸­
                        is_valid = any(vl['url'] == link['url'] for vl in valid_links)
                        
                        # é‡è¦ï¼šè®¾ç½®é“¾æ¥çš„is_validå±æ€§ï¼Œä»¥ä¾¿åç»­å¤„ç†
                        link['is_valid'] = is_valid
                        
                        # è·å–AIéªŒè¯åˆ†æ•°å’Œç†ç”±ï¼ˆå¦‚æœæœ‰ï¼‰
                        ai_score = 0
                        ai_reason = ""
                        ai_link_type = ""
                        
                        # ä»AIéªŒè¯æ—¥å¿—ä¸­æŸ¥æ‰¾æ­¤é“¾æ¥çš„è®°å½•
                        if ENABLE_LOGGING and os.path.exists(LOG_FILE):
                            try:
                                with open(LOG_FILE, "r", encoding="utf-8") as f:
                                    for line in f:
                                        try:
                                            record = json.loads(line)
                                            if record.get('url') == link['url']:
                                                ai_score = record.get('score', 0)
                                                ai_reason = record.get('reason', "")
                                                ai_link_type = "æ–‡ç« " if is_valid else "éæ–‡ç« "
                                                # å°†AIä¿¡æ¯æ·»åŠ åˆ°é“¾æ¥ä¿¡æ¯ä¸­
                                                link['ai_score'] = ai_score
                                                link['ai_reason'] = ai_reason
                                                link['link_type'] = ai_link_type
                                                break
                                        except json.JSONDecodeError:
                                            continue
                            except Exception as e:
                                print(f"è¯»å–AIéªŒè¯æ—¥å¿—æ—¶å‡ºé”™: {e}")
                        
                        # æ›´æ–°é“¾æ¥å†å² - æ­¤æ—¶åªæ›´æ–°éªŒè¯çŠ¶æ€ï¼Œå†…å®¹ç¨åçˆ¬å–
                        update_link_history(
                            url=link['url'],
                            title=link['title'],
                            source=source,
                            is_valid=is_valid,
                            quality_score=ai_score,
                            content_length=0,  # å†…å®¹é•¿åº¦ä¸º0ï¼Œè¡¨ç¤ºè¿˜æœªçˆ¬å–å†…å®¹
                            error_message="" if is_valid else "AIåˆ¤æ–­ä¸ºæ— æ•ˆé“¾æ¥",
                            ai_score=ai_score,
                            ai_reason=ai_reason,
                            link_type=ai_link_type
                        )
                elif not USE_AI_LINK_VALIDATION:
                    print("âš ï¸ æœªå¯ç”¨AIéªŒè¯ï¼Œæ‰€æœ‰æ–°é“¾æ¥éƒ½å°†è¢«è§†ä¸ºæœ‰æ•ˆ")
                    valid_links = new_links_to_validate
                else:
                    print("âš ï¸ AIéªŒè¯ä¸å¯ç”¨æˆ–æ²¡æœ‰æ–°é“¾æ¥éœ€è¦éªŒè¯")
                
                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                print(f"å…±æ‰¾åˆ° {len(valid_links)} ä¸ªæœ‰æ•ˆé“¾æ¥")
                
                # æ›´è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
                print(f"é“¾æ¥è¿‡æ»¤è¯¦æƒ…:")
                print(f" - åŸå§‹é“¾æ¥æ€»æ•°: {len(all_links)}")
                print(f" - è·³è¿‡å·²çŸ¥æœ‰æ•ˆé“¾æ¥: {skipped_known_valid_count}")
                print(f" - è·³è¿‡å·²çŸ¥æ— æ•ˆé“¾æ¥: {skipped_invalid_count}")
                print(f" - è·³è¿‡å·²å¤„ç†è¿‡å†…å®¹çš„é“¾æ¥: {skipped_already_processed}")
                print(f" - ç»è¿‡ç­›é€‰åçš„é“¾æ¥: {len(all_processed_links)}")
                print(f" - éœ€è¦éªŒè¯çš„é“¾æ¥: {len(new_links_to_validate)}")
                print(f" - æœ€ç»ˆæœ‰æ•ˆé“¾æ¥: {len(valid_links)}")
                
                # æ·»åŠ å¤„ç†çŠ¶æ€æ ‡è®°å’Œå…¶ä»–ä¿¡æ¯åˆ°æœ‰æ•ˆé“¾æ¥
                for vl in valid_links:
                    # æ ‡è®°ä¸ºéœ€è¦å¤„ç†
                    vl['need_process'] = not vl.get('is_processed', False)
                
                # ä¼˜å…ˆå¤„ç†æ–°é“¾æ¥ï¼Œæœ€å¤šè¿”å›MAX_LINKS_PER_SOURCEä¸ª
                return valid_links[:MAX_LINKS_PER_SOURCE]
            except Exception as e:
                print(f"å¤„ç†é“¾æ¥æ—¶å‡ºé”™: {e}")
                traceback.print_exc()
                return []
    except Exception as e:
        print(f"çˆ¬å–ä¸»é¡µæ—¶å‡ºé”™: {e}")
        traceback.print_exc()
        return []

# å¯¼å‡ºå†å²é“¾æ¥åº“åˆ°Excelï¼ˆæ–°å¢å‡½æ•°ï¼‰
def export_links_history_to_excel():
    """å°†æ‰€æœ‰å†å²é“¾æ¥æ•°æ®å¯¼å‡ºåˆ°Excelæ–‡ä»¶"""
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"links_history_{timestamp}.xlsx"
        
        # åˆ›å»ºå·¥ä½œç°¿å’Œå·¥ä½œè¡¨
        wb = openpyxl.Workbook()
        sheet = wb.active
        sheet.title = "å†å²é“¾æ¥"
        
        # è®¾ç½®è¡¨å¤´
        headers = [
            'åºå·', 'é“¾æ¥ID', 'æ¥æº', 'URL', 'æ ‡é¢˜', 'æœ‰æ•ˆæ€§', 'é¦–æ¬¡å‘ç°æ—¶é—´', 
            'æœ€åæ›´æ–°æ—¶é—´', 'å†…å®¹é•¿åº¦', 'è´¨é‡è¯„åˆ†', 'çˆ¬å–æ¬¡æ•°', 'AIéªŒè¯åˆ†æ•°', 'AIéªŒè¯ç†ç”±', 'é“¾æ¥ç±»å‹'
        ]
        
        # è®¾ç½®åˆ—å®½
        column_widths = {
            'A': 8,   # åºå·
            'B': 35,  # é“¾æ¥ID
            'C': 15,  # æ¥æº
            'D': 60,  # URL
            'E': 50,  # æ ‡é¢˜
            'F': 10,  # æœ‰æ•ˆæ€§
            'G': 20,  # é¦–æ¬¡å‘ç°æ—¶é—´
            'H': 20,  # æœ€åæ›´æ–°æ—¶é—´
            'I': 12,  # å†…å®¹é•¿åº¦
            'J': 12,  # è´¨é‡è¯„åˆ†
            'K': 10,  # çˆ¬å–æ¬¡æ•°
            'L': 12,  # AIéªŒè¯åˆ†æ•°
            'M': 40,  # AIéªŒè¯ç†ç”±
            'N': 15,  # é“¾æ¥ç±»å‹
        }
        
        for col, width in column_widths.items():
            sheet.column_dimensions[col].width = width
        
        # å†™å…¥è¡¨å¤´
        for idx, header in enumerate(headers, 1):
            sheet.cell(row=1, column=idx).value = header
        
        # éå†æ‰€æœ‰å†å²è®°å½•æ–‡ä»¶
        row_idx = 2
        total_records = 0
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(LINKS_HISTORY_DIR):
            print(f"å†å²é“¾æ¥ç›®å½•ä¸å­˜åœ¨: {LINKS_HISTORY_DIR}")
            return None
            
        # è·å–æ‰€æœ‰å†å²è®°å½•æ–‡ä»¶
        history_files = [f for f in os.listdir(LINKS_HISTORY_DIR) if f.endswith('_links.json')]
        
        for history_file in history_files:
            # æå–æ¥æºID
            source = history_file.replace('_links.json', '')
            
            # åŠ è½½æ•°æ®
            file_path = os.path.join(LINKS_HISTORY_DIR, history_file)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    links_data = json.load(f)
                    
                # å†™å…¥æ•°æ®
                for link_id, info in links_data.items():
                    # æ„å»ºæ•°æ®è¡Œ
                    data = [
                        row_idx - 1,  # åºå·
                        link_id,      # é“¾æ¥ID
                        source,       # æ¥æº
                        info.get('url', ''),            # URL
                        info.get('title', ''),          # æ ‡é¢˜
                        'æœ‰æ•ˆ' if info.get('is_valid', False) else 'æ— æ•ˆ',  # æœ‰æ•ˆæ€§
                        info.get('first_seen', ''),     # é¦–æ¬¡å‘ç°æ—¶é—´
                        info.get('last_updated', ''),   # æœ€åæ›´æ–°æ—¶é—´
                        info.get('content_length', 0),  # å†…å®¹é•¿åº¦
                        info.get('quality_score', 0),   # è´¨é‡è¯„åˆ†
                        info.get('crawl_count', 0),     # çˆ¬å–æ¬¡æ•°
                        info.get('ai_score', 0),        # AIéªŒè¯åˆ†æ•°
                        info.get('ai_reason', ''),      # AIéªŒè¯ç†ç”±
                        info.get('link_type', '')       # é“¾æ¥ç±»å‹
                    ]
                    
                    # å†™å…¥ä¸€è¡Œæ•°æ®
                    for col, value in enumerate(data, 1):
                        sheet.cell(row=row_idx, column=col).value = value
                        
                    row_idx += 1
                    total_records += 1
                    
            except Exception as e:
                print(f"å¤„ç†å†å²æ–‡ä»¶æ—¶å‡ºé”™ {history_file}: {e}")
                continue
        
        # ä¿å­˜æ–‡ä»¶
        wb.save(filename)
        print(f"æˆåŠŸå¯¼å‡ºå†å²é“¾æ¥æ•°æ®åˆ° {filename}ï¼Œå…± {total_records} æ¡è®°å½•")
        return filename
        
    except Exception as e:
        print(f"å¯¼å‡ºå†å²é“¾æ¥æ•°æ®æ—¶å‡ºé”™: {e}")
        return None

# å°†æŠ“å–ç»“æœå†™å…¥Excelæ–‡ä»¶
def write_result_to_excel(url, title, source_name, publish_date, content, link_duration, 
                          is_valid_content, retry_count, processed_count, ai_score=0, 
                          ai_reason="", link_type=""):
    """å°†æŠ“å–ç»“æœå†™å…¥Excelæ–‡ä»¶"""
    if not SAVE_TO_EXCEL or not result_excel_file or not os.path.exists(result_excel_file):
        return
    
    try:
        # åŠ è½½Excelæ–‡ä»¶
        wb = openpyxl.load_workbook(result_excel_file)
        sheet = wb.active
        next_row = sheet.max_row + 1
        
        # æ„å»ºæ•°æ®
        data = [
            processed_count,  # ç´¢å¼•
            source_name,      # æ¥æº
            url,              # é“¾æ¥
            title,            # æ ‡é¢˜
            publish_date,     # å‘å¸ƒæ—¥æœŸ
            "æ–°é—»" if is_valid_content else "éæ–°é—»",  # åˆ†ç±»
            len(content),     # æ­£æ–‡å­—æ•°
            f"{link_duration:.2f}",    # çˆ¬å–æ—¶é—´
            "æˆåŠŸ" if is_valid_content else "å¤±è´¥",  # çŠ¶æ€
            retry_count,      # å°è¯•æ¬¡æ•°
            ai_score,         # AIéªŒè¯åˆ†æ•°
            ai_reason,        # AIéªŒè¯ç†ç”±
            link_type         # é“¾æ¥ç±»å‹
        ]
        
        # å†™å…¥æ•°æ®
        for col, value in enumerate(data, 1):
            sheet.cell(row=next_row, column=col).value = value
            
        # ä¿å­˜
        wb.save(result_excel_file)
        print(f"âœ… å·²å°†ç»“æœå†™å…¥Excel: è¡Œ {next_row}")
    except Exception as e:
        print(f"âŒ å†™å…¥Excelæ—¶å‡ºé”™: {e}")

# ä¸»ç¨‹åº
async def main():
    """ä¸»ç¨‹åºå…¥å£"""
    # åˆ›å»ºç»“æœExcelæ–‡ä»¶
    if SAVE_TO_EXCEL:
        result_file = create_result_excel()
        print(f"ğŸ“Š Excelç»“æœå°†ä¿å­˜åˆ°: {result_excel_file}")
    
    print(f"ğŸš€ å¼€å§‹æ–°é—»çˆ¬å–ï¼Œæ‰¹æ¬¡ID: {batch_id}")
    
    # è¯»å–excelæ–‡ä»¶ä¸­çš„ä¸»é¡µé“¾æ¥
    excel_source_file = r'C:\Python\github\Crawl4AI\testhomepage.xlsx'
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(excel_source_file):
        print(f"âš ï¸ æºæ–‡ä»¶ä¸å­˜åœ¨: {excel_source_file}")
        excel_source_file = "../testhomepage.xlsx"
        if not os.path.exists(excel_source_file):
            print(f"âš ï¸ å¤‡ç”¨æºæ–‡ä»¶ä¹Ÿä¸å­˜åœ¨: {excel_source_file}")
            return
    
    print(f"ğŸ“„ æ‰¾åˆ°æºæ–‡ä»¶: {excel_source_file}")
    wb_source = openpyxl.load_workbook(excel_source_file)
    sheet_source = wb_source.active
    
    # åˆ›å»ºå†å²é“¾æ¥ç›®å½•
    os.makedirs(LINKS_HISTORY_DIR, exist_ok=True)
    
    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
    source_count = 0         # å¤„ç†çš„æ¥æºæ•°é‡
    processed_count = 0      # å¤„ç†çš„é“¾æ¥æ•°é‡
    success_count = 0        # æˆåŠŸå¤„ç†çš„é“¾æ¥æ•°é‡
    error_count = 0          # å¤„ç†å¤±è´¥çš„é“¾æ¥æ•°é‡
    skipped_count = 0        # è·³è¿‡çš„é“¾æ¥æ•°é‡
    new_link_count = 0       # æ–°é“¾æ¥æ•°é‡
    invalid_link_count = 0   # æ— æ•ˆé“¾æ¥æ•°é‡
    
    # éå†excelæ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œ
    for row in range(2, sheet_source.max_row + 1):  # ä»ç¬¬2è¡Œå¼€å§‹ï¼Œè·³è¿‡æ ‡é¢˜è¡Œ
        try:
            remark = sheet_source.cell(row=row, column=1).value
            main_url = sheet_source.cell(row=row, column=2).value
            source_name = sheet_source.cell(row=row, column=3).value
            
            if not main_url or not source_name:
                continue
                
            source_count += 1
            
            print(f"\nğŸŒ å¤„ç†æ¥æº {source_count}: {remark} - {main_url} (Source ID: {source_name})")
            
            # åŠ è½½æ­¤æºçš„å†å²é“¾æ¥
            links_history_file = os.path.join(LINKS_HISTORY_DIR, f"{source_name}_links.json")
            print(f"ğŸ“‹ å†å²é“¾æ¥æ–‡ä»¶: {links_history_file}")
            
            # çˆ¬å–ä¸»é¡µä¸­çš„æ–°é—»é“¾æ¥
            start_time = time.time()
            links = await fetch_news_links(main_url, source_name)
            end_time = time.time()
            
            print(f"â±ï¸ çˆ¬å–é“¾æ¥è€—æ—¶: {end_time - start_time:.2f}ç§’")
            
            # å¤„ç†æ¯ä¸ªé“¾æ¥
            source_success_count = 0
            for link_info in links:
                processed_count += 1
                retry_count = 0
                
                # æ£€æŸ¥é“¾æ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡å†…å®¹
                if link_info.get('is_processed', False):
                    print(f"\nâ© è·³è¿‡å·²å¤„ç†è¿‡å†…å®¹çš„é“¾æ¥: {link_info['url']}")
                    skipped_count += 1
                    continue
                
                # è®°å½•æ˜¯å¦ä¸ºæ–°é“¾æ¥
                if link_info.get('is_new', True):
                    new_link_count += 1
                
                # åªæœ‰AIåˆ¤å®šçš„æœ‰æ•ˆé“¾æ¥æ‰è¿›è¡Œå†…å®¹çˆ¬å–
                if 'is_valid' in link_info and link_info['is_valid']:
                    while retry_count <= MAX_RETRY_COUNT:
                        try:
                            url = link_info['url']
                            title_from_link = link_info['title']
                            is_new, _, is_processed = is_new_link(url, source_name)
                            
                            # å¦‚æœå·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡
                            if is_processed:
                                print(f"\nâ© è·³è¿‡å·²å¤„ç†è¿‡å†…å®¹çš„é“¾æ¥: {url}")
                                break
                                
                            crawl_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            
                            print(f"\nğŸ”— æ­£åœ¨å¤„ç†é“¾æ¥: {url}")
                            print(f"ğŸ“Œ é“¾æ¥æ ‡é¢˜: {title_from_link}")
                            print(f"ğŸ†• æ˜¯å¦æ–°é“¾æ¥: {'æ˜¯' if is_new else 'å¦'}")
                            
                            # è®¾ç½®æµè§ˆå™¨é…ç½®
                            browser_config = BrowserConfig(
                                browser_type="chromium",
                                headless=True, 
                                viewport_width=1366,
                                viewport_height=768,
                                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
                                java_script_enabled=True,
                                ignore_https_errors=True
                            )
                            
                            # è®¾ç½®çˆ¬è™«é…ç½®
                            prune_filter = PruningContentFilter(
                                threshold=0.45,
                                threshold_type="dynamic",
                                min_word_threshold=5
                            )
                            md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)
                            
                            crawler_config = CrawlerRunConfig(
                                markdown_generator=md_generator,
                                page_timeout=REQUEST_TIMEOUT * 1000,
                                cache_mode=CacheMode.BYPASS
                            )
                            
                            # çˆ¬å–å†…å®¹
                            link_start_time = time.time()
                            async with AsyncWebCrawler(config=browser_config) as crawler:
                                result = await crawler.arun(url=url, config=crawler_config)
                            link_end_time = time.time()
                            
                            # è®¡ç®—çˆ¬å–è€—æ—¶
                            link_duration = link_end_time - link_start_time
                            
                            if result.success:
                                print(f"âœ… æˆåŠŸè·å–å†…å®¹, è€—æ—¶: {link_duration:.2f}ç§’")
                                
                                # ä½¿ç”¨BeautifulSoupå¤„ç†HTML
                                soup = BeautifulSoup(result.html, 'html.parser')
                                title = extract_title(soup, url) or title_from_link
                                publish_date = extract_publish_date(soup) or "æœªæ‰¾åˆ°æ—¥æœŸ"
                                content = result.markdown.fit_markdown
                                content_length = len(content)
                                
                                print(f"ğŸ“ æ ‡é¢˜: {title}")
                                print(f"ğŸ“… å‘å¸ƒæ—¥æœŸ: {publish_date}")
                                print(f"ğŸ“Š å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")
                                
                                # ä½¿ç”¨evaluate_content_qualityè·å–æ‘˜è¦å’ŒæŒ‡çº¹ï¼ˆä¸å†è¯„ä¼°å†…å®¹è´¨é‡ï¼‰
                                _, content_summary, content_fingerprint = evaluate_content_quality(result.html, title, url)
                                
                                # AIéªŒè¯é€šè¿‡çš„é“¾æ¥éƒ½è§†ä¸ºæœ‰æ•ˆ
                                is_valid_content = True
                                
                                # è¾“å‡ºç»“æœ
                                print(f"âœ… æˆåŠŸè·å–å†…å®¹, è€—æ—¶: {link_duration:.2f}ç§’")
                                print(f"ğŸ’¡ å†…å®¹æ‘˜è¦:\n{content_summary}")
                                
                                # çˆ¬å–å†…å®¹åï¼Œæ›´æ–°é“¾æ¥å†å²è®°å½•ï¼Œè®¾ç½®å†…å®¹é•¿åº¦ä»¥æ ‡è®°ä¸ºå·²å¤„ç†
                                link_info['is_processed'] = True
                                
                                # è·å–AIéªŒè¯ä¿¡æ¯
                                ai_score = link_info.get('ai_score', 0)
                                ai_reason = link_info.get('ai_reason', "")
                                link_type = link_info.get('link_type', "")
                                
                                # æ›´æ–°é“¾æ¥å†å²
                                update_link_history(
                                    url=url,
                                    title=title,
                                    source=source_name,
                                    content_summary=content_summary,
                                    is_valid=is_valid_content,
                                    quality_score=ai_score,
                                    content_length=content_length,
                                    content_fingerprint=content_fingerprint,
                                    ai_score=ai_score,
                                    ai_reason=ai_reason,
                                    link_type=link_type
                                )
                                
                                # å†™å…¥Excel
                                if SAVE_TO_EXCEL:
                                    write_result_to_excel(
                                        url=url,
                                        title=title,
                                        source_name=source_name,
                                        publish_date=publish_date,
                                        content=content,
                                        link_duration=link_duration,
                                        is_valid_content=is_valid_content,
                                        retry_count=retry_count,
                                        processed_count=processed_count,
                                        ai_score=ai_score,
                                        ai_reason=ai_reason,
                                        link_type=link_type
                                    )
                                
                                source_success_count += 1
                                success_count += 1
                                break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                                
                            else:
                                print(f"âŒ è·å–å†…å®¹å¤±è´¥: {result.error_message}")
                                error_message = result.error_message
                                
                                # æ›´æ–°é“¾æ¥å†å²è®°å½• - æ ‡è®°ä¸ºè·å–å¤±è´¥
                                update_link_history(
                                    url=url,
                                    title=title_from_link,
                                    source=source_name,
                                    is_valid=False,
                                    error_message=f"çˆ¬å–å¤±è´¥: {error_message}"
                                )
                                
                                retry_count += 1
                                if retry_count <= MAX_RETRY_COUNT:
                                    print(f"â³ é‡è¯• ({retry_count}/{MAX_RETRY_COUNT})...")
                                else:
                                    print(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒæ­¤é“¾æ¥")
                                    error_count += 1
                                    
                        except Exception as e:
                            print(f"âŒ å¤„ç†é“¾æ¥æ—¶å‡ºé”™: {e}")
                            traceback.print_exc()
                            
                            # æ›´æ–°é“¾æ¥å†å²è®°å½• - æ ‡è®°ä¸ºå¤„ç†å¼‚å¸¸
                            update_link_history(
                                url=url,
                                title=title_from_link,
                                source=source_name,
                                is_valid=False,
                                error_message=f"å¤„ç†å¼‚å¸¸: {str(e)}"
                            )
                            
                            retry_count += 1
                            if retry_count <= MAX_RETRY_COUNT:
                                print(f"â³ é‡è¯• ({retry_count}/{MAX_RETRY_COUNT})...")
                            else:
                                print(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒæ­¤é“¾æ¥")
                                error_count += 1
                else:
                    # å¯¹äºæ— æ•ˆé“¾æ¥ï¼Œåªæ›´æ–°å†å²è®°å½•ï¼Œä¸çˆ¬å–å†…å®¹
                    url = link_info['url']
                    title_from_link = link_info['title']
                    invalid_link_count += 1
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºå·²çŸ¥çš„æ— æ•ˆé“¾æ¥
                    is_new, is_invalid, is_processed = is_new_link(url, source_name)
                    if not is_new and is_invalid:
                        print(f"\nğŸ”— è·³è¿‡å·²çŸ¥æ— æ•ˆé“¾æ¥: {url}")
                        skipped_count += 1
                        continue
                        
                    print(f"\nğŸ”— è®°å½•æ–°çš„æ— æ•ˆé“¾æ¥: {url}")
                    
                    # æ›´æ–°é“¾æ¥å†å²ï¼Œæ ‡è®°ä¸ºæ— æ•ˆ
                    update_link_history(
                        url=url,
                        title=title_from_link,
                        source=source_name,
                        is_valid=False,
                        quality_score=link_info.get('ai_score', 0),
                        error_message="AIåˆ¤æ–­ä¸ºæ— æ•ˆé“¾æ¥"
                    )
                
            # å®Œæˆå½“å‰æ¥æºçš„å¤„ç†
            print(f"\nâœ… æ¥æº {source_name} å¤„ç†å®Œæˆï¼ŒæˆåŠŸè·å– {source_success_count} ä¸ªæœ‰æ•ˆé“¾æ¥")
            
        except Exception as e:
            print(f"âŒ å¤„ç†æº {source_name} æ—¶å‡ºé”™: {e}")
            traceback.print_exc()
            continue
            
    # è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯æ‰“å°
    print(f"\nğŸ“Š çˆ¬å–ç»Ÿè®¡ä¿¡æ¯:")
    print(f"æ€»å…±å¤„ç†äº† {source_count} ä¸ªæ¥æº")
    print(f"æ€»å…±å‘ç°äº† {processed_count + skipped_count} ä¸ªé“¾æ¥")
    print(f" - å…¶ä¸­ {new_link_count} ä¸ªæ˜¯æ–°é“¾æ¥")
    print(f" - å…¶ä¸­ {skipped_count} ä¸ªé“¾æ¥è¢«è·³è¿‡ï¼ˆå·²å¤„ç†è¿‡æˆ–å·²çŸ¥æ— æ•ˆï¼‰")
    print(f"å®é™…å¤„ç†äº† {processed_count} ä¸ªé“¾æ¥")
    print(f" - å…¶ä¸­ {success_count} ä¸ªæœ‰æ•ˆå¹¶æˆåŠŸçˆ¬å–")
    print(f" - å…¶ä¸­ {error_count} ä¸ªå¤„ç†å¤±è´¥")
    print(f" - å…¶ä¸­ {invalid_link_count} ä¸ªè¢«AIåˆ¤æ–­ä¸ºæ— æ•ˆ")
    
    # è®¡ç®—æœ‰æ•ˆç‡
    if processed_count > 0:
        success_rate = (success_count / processed_count) * 100
        print(f"æœ‰æ•ˆç‡: {success_rate:.2f}%")
    
    # å¯¼å‡ºå†å²é“¾æ¥åˆ°Excel
    history_file = export_links_history_to_excel()
    if history_file:
        print(f"ğŸ“Š å†å²é“¾æ¥å·²å¯¼å‡ºåˆ°: {history_file}")
    
    print(f"ğŸ ç¨‹åºç»“æŸï¼Œæ‰¹æ¬¡ID: {batch_id}")

# å½“ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ—¶æ‰§è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == "--debug":
        # è°ƒè¯•æ¨¡å¼ï¼šçˆ¬å–å•ä¸ªé“¾æ¥
        if len(sys.argv) >= 4:
            # æ ¼å¼: python test_request3.py --debug <url> <source_name>
            debug_url = sys.argv[2]
            debug_source = sys.argv[3]
            
            print(f"ğŸ” è°ƒè¯•æ¨¡å¼: çˆ¬å–å•ä¸ªé“¾æ¥")
            print(f"ğŸ”— URL: {debug_url}")
            print(f"ğŸ“Œ æ¥æº: {debug_source}")
            
            # åˆ›å»ºç»“æœExcelæ–‡ä»¶
            if SAVE_TO_EXCEL:
                result_file = create_result_excel()
                print(f"ğŸ“Š Excelç»“æœå°†ä¿å­˜åˆ°: {result_excel_file}")
            
            # ç¡®ä¿å†å²é“¾æ¥ç›®å½•å­˜åœ¨
            os.makedirs(LINKS_HISTORY_DIR, exist_ok=True)
            
            # å®šä¹‰å•ä¸ªé“¾æ¥è°ƒè¯•å‡½æ•°
            async def debug_single_link():
                try:
                    # åˆ›å»ºçˆ¬è™«é…ç½®
                    browser_config = BrowserConfig(
                        browser_type="chromium",
                        viewport_width=1280, 
                        viewport_height=800,
                        java_script_enabled=True,
                        ignore_https_errors=True
                    )
                    
                    # è®¾ç½®è¿‡æ»¤å™¨
                    prune_filter = PruningContentFilter(
                        threshold=0.45,
                        threshold_type="dynamic",
                        min_word_threshold=5
                    )
                    
                    # è®¾ç½®Markdownç”Ÿæˆå™¨
                    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)
                    
                    # çˆ¬è™«é…ç½®
                    crawler_config = CrawlerRunConfig(
                        markdown_generator=md_generator,
                        page_timeout=REQUEST_TIMEOUT * 1000,
                        cache_mode=CacheMode.BYPASS
                    )
                    
                    # å…ˆéªŒè¯é“¾æ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ–°é—»é“¾æ¥
                    print(f"ğŸ§  ä½¿ç”¨AIéªŒè¯é“¾æ¥æœ‰æ•ˆæ€§...")
                    parsed = urlparse(debug_url)
                    base_url = f"{parsed.scheme}://{parsed.netloc}"
                    
                    if AI_LINK_VALIDATOR_AVAILABLE:
                        # ä½¿ç”¨AIéªŒè¯
                        is_valid = ai_link_validator.is_valid_news_link_with_ai(debug_url, base_url)
                        if not is_valid:
                            print(f"âŒ AIåˆ¤æ–­è¯¥é“¾æ¥ä¸æ˜¯æœ‰æ•ˆçš„æ–°é—»é“¾æ¥ï¼Œä½†ä»å°†å°è¯•çˆ¬å–å†…å®¹")
                    else:
                        print(f"âš ï¸ AIéªŒè¯æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ç›´æ¥çˆ¬å–å†…å®¹")
                    
                    print(f"ğŸ”„ å¼€å§‹çˆ¬å–é“¾æ¥: {debug_url}")
                    start_time = time.time()
                    
                    # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶çˆ¬å–å†…å®¹
                    async with AsyncWebCrawler() as crawler:
                        result = await crawler.arun(url=debug_url, config=crawler_config)
                    
                    end_time = time.time()
                    duration = end_time - start_time
                    
                    if result.success:
                        # è§£æå†…å®¹
                        soup = BeautifulSoup(result.html, 'html.parser')
                        title = extract_title(soup, debug_url)
                        publish_date = extract_publish_date(soup) or "æœªæ‰¾åˆ°æ—¥æœŸ"
                        content = result.markdown.fit_markdown if hasattr(result.markdown, 'fit_markdown') else ""
                        content_length = len(content)
                        
                        # ä½¿ç”¨evaluate_content_qualityè·å–æ‘˜è¦å’ŒæŒ‡çº¹ï¼ˆä¸å†è¯„ä¼°å†…å®¹è´¨é‡ï¼‰
                        _, content_summary, content_fingerprint = evaluate_content_quality(result.html, title, debug_url)
                        
                        # è·å–AIéªŒè¯ä¿¡æ¯
                        ai_score = 0
                        ai_reason = ""
                        link_type = ""
                        
                        # ä»AIéªŒè¯æ—¥å¿—ä¸­æŸ¥æ‰¾æ­¤é“¾æ¥çš„è®°å½•
                        if ENABLE_LOGGING and os.path.exists(LOG_FILE):
                            try:
                                with open(LOG_FILE, "r", encoding="utf-8") as f:
                                    for line in f:
                                        try:
                                            record = json.loads(line)
                                            if record.get('url') == debug_url:
                                                ai_score = record.get('score', 0)
                                                ai_reason = record.get('reason', "")
                                                link_type = "æ–‡ç« " if record.get('is_valid', False) else "éæ–‡ç« "
                                                break
                                        except json.JSONDecodeError:
                                            continue
                            except Exception as e:
                                print(f"è¯»å–AIéªŒè¯æ—¥å¿—æ—¶å‡ºé”™: {e}")
                        
                        # è¾“å‡ºç»“æœ
                        print(f"\nâœ… çˆ¬å–æˆåŠŸ! è€—æ—¶: {duration:.2f}ç§’")
                        print(f"ğŸ“ æ ‡é¢˜: {title}")
                        print(f"ğŸ“… å‘å¸ƒæ—¥æœŸ: {publish_date}")
                        print(f"ğŸ“Š å†…å®¹é•¿åº¦: {content_length}å­—ç¬¦")
                        if ai_score > 0:
                            print(f"ğŸ§  AIéªŒè¯åˆ†æ•°: {ai_score}/100")
                            print(f"ğŸ§  AIéªŒè¯ç†ç”±: {ai_reason}")
                        print(f"ğŸ’¡ å†…å®¹æ‘˜è¦:\n{content_summary}")
                        
                        # æ›´æ–°å†å²é“¾æ¥
                        update_link_history(
                            url=debug_url,
                            title=title,
                            source=debug_source,
                            content_summary=content_summary,
                            is_valid=True,  # è°ƒè¯•æ¨¡å¼ä¸‹å§‹ç»ˆè§†ä¸ºæœ‰æ•ˆ
                            quality_score=ai_score,
                            content_length=content_length,
                            content_fingerprint=content_fingerprint,
                            ai_score=ai_score,
                            ai_reason=ai_reason,
                            link_type=link_type
                        )
                        
                        # å†™å…¥Excelç»“æœ
                        if SAVE_TO_EXCEL:
                            write_result_to_excel(
                                url=debug_url,
                                title=title,
                                source_name=debug_source,
                                publish_date=publish_date,
                                content=content,
                                link_duration=duration,
                                is_valid_content=True,
                                retry_count=0,
                                processed_count=1,
                                ai_score=ai_score,
                                ai_reason=ai_reason,
                                link_type=link_type
                            )
                        
                        # æ‰“å°éƒ¨åˆ†å†…å®¹
                        print("\nğŸ“„ å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦):")
                        print(f"{content[:500]}...")
                        print(f"\nå®Œæ•´å†…å®¹å·²ä¿å­˜åˆ°Excelæ–‡ä»¶: {result_excel_file}")
                        
                    else:
                        print(f"\nâŒ çˆ¬å–å¤±è´¥! è€—æ—¶: {duration:.2f}ç§’")
                        print(f"é”™è¯¯ä¿¡æ¯: {result.error_message}")
                
                except Exception as e:
                    print(f"\nâš ï¸ è°ƒè¯•æ¨¡å¼å‡ºé”™: {str(e)}")
                    traceback.print_exc()
            
            # æ‰§è¡Œå•é“¾æ¥è°ƒè¯•
            asyncio.run(debug_single_link())
            
        else:
            print("âŒ è°ƒè¯•æ¨¡å¼ç”¨æ³•: python test_request3.py --debug <url> <source_name>")
            print("ä¾‹å¦‚: python test_request3.py --debug https://example.com/news/article123 source1")
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šè¿è¡Œå®Œæ•´çˆ¬å–æµç¨‹
        asyncio.run(main()) 