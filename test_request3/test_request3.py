import openpyxl
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode
from bs4 import BeautifulSoup
import os
import json
import re
import traceback
from datetime import datetime
from urllib.parse import urlparse, urljoin, urlunparse
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawl4ai import CacheMode
import requests
import pandas as pd
import time
import hashlib
from concurrent.futures import ThreadPoolExecutor
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter
import random

# å¯¼å…¥AIé“¾æ¥éªŒè¯æ¨¡å—
try:
    # å°è¯•ä»åŒçº§ç›®å½•å¯¼å…¥
    from ai_link_validator import is_valid_news_link_with_ai, batch_link_validation
    AI_LINK_VALIDATOR_AVAILABLE = True
except ImportError:
    try:
        # å°è¯•ä»ä¸Šçº§ç›®å½•å¯¼å…¥
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from ai_link_validator import is_valid_news_link_with_ai, batch_link_validation
        AI_LINK_VALIDATOR_AVAILABLE = True
    except ImportError:
        print("âš ï¸ AIé“¾æ¥éªŒè¯æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•åˆ¤æ–­é“¾æ¥æœ‰æ•ˆæ€§")
        AI_LINK_VALIDATOR_AVAILABLE = False

# é…ç½®é€‰é¡¹
SAVE_TO_EXCEL = True                # æ˜¯å¦ä¿å­˜ç»“æœåˆ°Excel
MAX_LINKS_PER_SOURCE = 20           # æ¯ä¸ªæ¥æºæœ€å¤šæŠ“å–çš„é“¾æ¥æ•°
MIN_CONTENT_LENGTH = 300            # æœ€å°æœ‰æ•ˆå†…å®¹é•¿åº¦
MAX_RETRY_COUNT = 2                 # é“¾æ¥è¯·æ±‚å¤±è´¥æ—¶æœ€å¤§é‡è¯•æ¬¡æ•°
REQUEST_TIMEOUT = 60                # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
SKIP_EXISTING_LINKS = True          # æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„é“¾æ¥
LINKS_HISTORY_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "link_history")  # å†å²é“¾æ¥å­˜å‚¨ç›®å½•
USE_AI_LINK_VALIDATION = True       # æ˜¯å¦ä½¿ç”¨AIé“¾æ¥éªŒè¯
USE_COLD_START = False               # å†·å¯åŠ¨æ¨¡å¼ï¼šåªæ”¶é›†é“¾æ¥åˆ°å†å²è®°å½•ï¼Œä¸è¿›è¡Œå†…å®¹çˆ¬å–ï¼ˆé€‚ç”¨äºé¦–æ¬¡é‡åˆ°å¤§é‡é“¾æ¥æ—¶ï¼‰

# ç®€åŒ–è¯„åˆ†å‚æ•° - ä»…ä¿ç•™åŸºç¡€è®¾ç½®
QUALITY_CONFIG = {
    "min_content_length": 300,    # æœ€å°æœ‰æ•ˆå†…å®¹é•¿åº¦
    "min_paragraphs": 3           # æœ€å°‘æ®µè½æ•°
}

# ç¡®ä¿å†å²ç›®å½•å­˜åœ¨
if not os.path.exists(LINKS_HISTORY_DIR):
    os.makedirs(LINKS_HISTORY_DIR)

# ä½¿ç”¨URLä½œä¸ºé“¾æ¥å”¯ä¸€æ ‡è¯†
def generate_link_id(url):
    """ç”Ÿæˆé“¾æ¥çš„å”¯ä¸€æ ‡è¯†ï¼Œç›´æ¥ä½¿ç”¨URLä½œä¸ºæ ‡è¯†"""
    return url

# æ·»åŠ URLè§„èŒƒåŒ–å‡½æ•°
def normalize_url(url):
    """è§„èŒƒåŒ–URLä»¥ä¾¿è¿›è¡Œæ¯”è¾ƒï¼Œç§»é™¤æœ«å°¾æ–œæ å’Œwww.å‰ç¼€ç­‰ï¼Œä¼˜åŒ–é«˜é¢‘è°ƒç”¨æ€§èƒ½"""
    if not url:
        return ""
    
    try:
        # å¿«é€Ÿæ£€æµ‹æ˜¯å¦ä¸ºURLï¼Œé¿å…ä¸å¿…è¦çš„è§£æ
        if not any(p in url for p in ('http://', 'https://', 'www.')):
            return url.strip().rstrip('/')
        
        # è§£æURL
        parsed = urlparse(url)
        
        # è·å–åŸŸåå’Œè·¯å¾„éƒ¨åˆ†
        netloc = parsed.netloc
        path = parsed.path
        
        # ç§»é™¤www.å‰ç¼€
        if netloc.startswith('www.'):
            netloc = netloc[4:]
        
        # ç§»é™¤æœ«å°¾æ–œæ 
        if path.endswith('/') and len(path) > 1:
            path = path[:-1]
        
        # é‡æ–°ç»„åˆURL
        # ä¼˜åŒ–: åªä¿ç•™scheme, netloc, pathéƒ¨åˆ†ï¼Œå¿½ç•¥queryå’Œfragment
        normalized = f"{parsed.scheme}://{netloc}{path}"
        
        return normalized
    except Exception:
        # å‡ºé”™æ—¶è¿”å›åŸå§‹URL
        return url.strip()

# æ£€æµ‹æ˜¯å¦ä¸ºæ–°é“¾æ¥å¹¶è¿½è¸ªé“¾æ¥çŠ¶æ€
def is_new_link(url, source):
    """
    æ£€æŸ¥é“¾æ¥æ˜¯å¦ä¸ºæ–°é“¾æ¥ï¼ˆæœªçˆ¬å–è¿‡ï¼‰å¹¶è·å–çŠ¶æ€
    
    è¿”å›: (is_new, is_invalid, is_processed)
    is_new: å¸ƒå°”å€¼ï¼ŒTrueè¡¨ç¤ºé“¾æ¥æœªçˆ¬å–è¿‡
    is_invalid: å¸ƒå°”å€¼ï¼ŒTrueè¡¨ç¤ºé“¾æ¥ä¹‹å‰å·²è¢«æ ‡è®°ä¸ºæ— æ•ˆ
    is_processed: å¸ƒå°”å€¼ï¼ŒTrueè¡¨ç¤ºé“¾æ¥å·²ç»è¢«å¤„ç†è¿‡ï¼ˆæ— è®ºæœ‰æ•ˆæˆ–æ— æ•ˆï¼‰
    """
    links_history = load_links_history(source)
    
    # ç›´æ¥æ£€æŸ¥URLæ˜¯å¦å­˜åœ¨äºå†å²è®°å½•ä¸­
    for _, info in links_history.items():
        if info.get('url', '') == url:
            is_valid = info.get('is_valid', True)
            # é“¾æ¥å­˜åœ¨ä¸”å·²ç»æŠ“å–è¿‡å†…å®¹ï¼ˆå†…å®¹é•¿åº¦å¤§äº0æˆ–çˆ¬å–æ¬¡æ•°å¤§äº1ï¼‰
            has_content = info.get('content_length', 0) > 0 or info.get('crawl_count', 0) > 1
            return False, not is_valid, has_content
    
    # URLä¸å­˜åœ¨äºå†å²è®°å½•ä¸­ï¼Œåˆ™ä¸ºæ–°é“¾æ¥ä¸”æœªå¤„ç†è¿‡
    return True, False, False

# åŠ è½½å†å²é“¾æ¥æ•°æ®
def load_links_history(source):
    """åŠ è½½æŒ‡å®šæ¥æºçš„å†å²é“¾æ¥æ•°æ®ï¼Œä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹å¼"""
    history_file = os.path.join(LINKS_HISTORY_DIR, f"{source}_links.json")
    links_data = {}
    
    try:
        if os.path.exists(history_file):
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ä»¥å†³å®šåŠ è½½æ–¹å¼
            file_size = os.path.getsize(history_file)
            
            with open(history_file, 'r', encoding='utf-8') as f:
                # å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹å¼è¯»å–
                if file_size > 10 * 1024 * 1024:  # å¤§äº10MBçš„æ–‡ä»¶
                    print(f"âš ï¸ å†å²é“¾æ¥æ–‡ä»¶è¾ƒå¤§ ({file_size/1024/1024:.2f}MB)ï¼Œä½¿ç”¨ä¼˜åŒ–åŠ è½½æ–¹å¼...")
                    links_data = json.load(f)
                else:
                    # å°æ–‡ä»¶åˆ™æ­£å¸¸åŠ è½½
                    links_data = json.load(f)
                    
            print(f"âœ… å·²åŠ è½½ {len(links_data)} æ¡å†å²é“¾æ¥: {history_file}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°å†å²é“¾æ¥æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°çš„è®°å½•: {history_file}")
    except Exception as e:
        print(f"âŒ åŠ è½½å†å²é“¾æ¥æ•°æ®å¤±è´¥: {str(e)}")
        traceback.print_exc()
    
    return links_data

# ä¿å­˜å†å²é“¾æ¥æ•°æ®
def save_links_history(source, links_data):
    """ä¿å­˜é“¾æ¥å†å²è®°å½•åˆ°æœ¬åœ°JSONæ–‡ä»¶ï¼Œä½¿ç”¨æ›´é«˜æ•ˆçš„æ‰¹é‡ä¿å­˜æ–¹å¼"""
    if not source or not links_data:
        return False
        
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    try:
        os.makedirs(LINKS_HISTORY_DIR, exist_ok=True)
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        file_path = os.path.join(LINKS_HISTORY_DIR, f"{source}_links.json")
        
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡Œå†™å…¥ï¼Œç„¶åé‡å‘½åï¼Œé¿å…æ–‡ä»¶æŸå
        temp_file = file_path + ".tmp"
        
        # ä¼˜åŒ–: å¦‚æœæ•°æ®ä¸è¶…è¿‡ä¸€å®šè§„æ¨¡ï¼Œç›´æ¥ä½¿ç”¨æ ‡å‡†JSONä¿å­˜
        if len(links_data) < 10000:  # å¯¹äºå°è§„æ¨¡æ•°æ®
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(links_data, f, ensure_ascii=False, indent=2)
        else:
            # å¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹å¼
            with open(temp_file, 'w', encoding='utf-8') as f:
                # ä¸ä½¿ç”¨ç¼©è¿›ï¼Œå‡å°‘æ–‡ä»¶å¤§å°å’Œä¿å­˜æ—¶é—´
                json.dump(links_data, f, ensure_ascii=False, separators=(',', ':'))
        
        # å®‰å…¨åœ°æ›¿æ¢åŸæ–‡ä»¶
        if os.path.exists(temp_file):
            # åœ¨Windowsç³»ç»Ÿä¸­ï¼Œå¦‚æœç›®æ ‡æ–‡ä»¶å­˜åœ¨éœ€è¦å…ˆåˆ é™¤
            if os.path.exists(file_path):
                os.remove(file_path)
            os.rename(temp_file, file_path)
            
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜é“¾æ¥å†å²è®°å½•å¤±è´¥: {str(e)}")
        traceback.print_exc()
        return False

# æ›´æ–°é“¾æ¥å†å²è®°å½•
def update_link_history(url, title, source, content_summary="", is_valid=True, quality_score=0, content_length=0, error_message="", content_fingerprint="", ai_score=0, ai_reason="", link_type=""):
    """
    æ›´æ–°é“¾æ¥å†å²è®°å½•
    
    å‚æ•°:
        url: é“¾æ¥URL
        title: é“¾æ¥æ ‡é¢˜
        source: æ¥æºID
        content_summary: å†…å®¹æ‘˜è¦
        is_valid: æ˜¯å¦æœ‰æ•ˆçš„é“¾æ¥
        quality_score: å†…å®¹è´¨é‡è¯„åˆ†
        content_length: å†…å®¹é•¿åº¦
        error_message: é”™è¯¯ä¿¡æ¯
        content_fingerprint: å†…å®¹æŒ‡çº¹
        ai_score: AIéªŒè¯åˆ†æ•°
        ai_reason: AIéªŒè¯ç†ç”±
        link_type: é“¾æ¥ç±»å‹
    """
    try:
        # åŠ è½½å†å²è®°å½•
        links_history = load_links_history(source)
        
        # ç”Ÿæˆé“¾æ¥ID
        link_id = generate_link_id(url)
        
        # å½“å‰æ—¶é—´
        current_time = datetime.now().isoformat()
        
        # æ£€æŸ¥æ­¤é“¾æ¥æ˜¯å¦å­˜åœ¨
        if link_id in links_history:
            # æ›´æ–°ç°æœ‰è®°å½•
            links_history[link_id]['is_valid'] = is_valid
            links_history[link_id]['last_updated'] = current_time
            
            # åªæœ‰åœ¨æä¾›äº†å†…å®¹æ—¶æ‰æ›´æ–°å†…å®¹ç›¸å…³å­—æ®µ
            if content_length > 0:
                links_history[link_id]['content_length'] = content_length
                if content_summary:
                    links_history[link_id]['content_summary'] = content_summary
                if content_fingerprint:
                    links_history[link_id]['content_fingerprint'] = content_fingerprint
            
            # æ›´æ–°è´¨é‡è¯„åˆ†ï¼ˆå¦‚æœæä¾›ï¼‰
            if quality_score > 0:
                links_history[link_id]['quality_score'] = quality_score
                
            # æ›´æ–°é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if error_message:
                links_history[link_id]['error_message'] = error_message
                
            # å¢åŠ çˆ¬å–æ¬¡æ•°
            links_history[link_id]['crawl_count'] = links_history[link_id].get('crawl_count', 0) + 1
                
            # æ›´æ–°AIéªŒè¯ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
            if ai_score > 0:
                links_history[link_id]['ai_score'] = ai_score
            if ai_reason:
                links_history[link_id]['ai_reason'] = ai_reason
            if link_type:
                links_history[link_id]['link_type'] = link_type
        else:
            # åˆ›å»ºæ–°è®°å½•
            links_history[link_id] = {
                'url': url,
                'title': title,
                'is_valid': is_valid,
                'first_seen': current_time,
                'last_updated': current_time,
                'content_length': content_length,
                'content_summary': content_summary,
                'quality_score': quality_score,
                'error_message': error_message,
                'content_fingerprint': content_fingerprint,
                'crawl_count': 1,  # åˆå§‹çˆ¬å–æ¬¡æ•°ä¸º1
                'ai_score': ai_score,
                'ai_reason': ai_reason,
                'link_type': link_type
            }
            
        # ä¿å­˜æ›´æ–°åçš„å†å²è®°å½•
        save_links_history(source, links_history)
        
    except Exception as e:
        print(f"æ›´æ–°é“¾æ¥å†å²è®°å½•å‡ºé”™: {e}")

# è¯»å–ä¸»ç•Œé¢é“¾æ¥
# ä½¿ç”¨å½“å‰è„šæœ¬ç›®å½•ä¸‹çš„testhomepage.xlsxæ–‡ä»¶
current_dir = os.path.dirname(os.path.abspath(__file__))
excel_source_file = os.path.join(current_dir, "testhomepage.xlsx")
# å¤‡é€‰ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœå½“å‰ç›®å½•ä¸‹ä¸å­˜åœ¨å¯ä»¥å°è¯•ä¸Šçº§ç›®å½•
alternative_path = '../testhomepage.xlsx'

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°è¯•å¤‡é€‰è·¯å¾„
if not os.path.exists(excel_source_file):
    print(f"âŒ ä¸»è·¯å¾„æ‰¾ä¸åˆ°æ–‡ä»¶: {excel_source_file}ï¼Œå°è¯•å¤‡é€‰è·¯å¾„...")
    if os.path.exists(alternative_path):
        excel_source_file = alternative_path
        print(f"âœ… æ‰¾åˆ°å¤‡é€‰è·¯å¾„æ–‡ä»¶: {excel_source_file}")
    else:
        print(f"âŒ å¤‡é€‰è·¯å¾„ä¹Ÿæ‰¾ä¸åˆ°æ–‡ä»¶: {alternative_path}")
        # å°è¯•åˆ—å‡ºå½“å‰ç›®å½•ä¸‹çš„Excelæ–‡ä»¶
        print("å½“å‰ç›®å½•ä¸‹çš„Excelæ–‡ä»¶:")
        for file in os.listdir('.'):
            if file.endswith('.xlsx'):
                print(f" - {file}")
        
        # å°è¯•åˆ—å‡ºä¸Šçº§ç›®å½•ä¸‹çš„Excelæ–‡ä»¶
        print("ä¸Šçº§ç›®å½•ä¸‹çš„Excelæ–‡ä»¶:")
        for file in os.listdir('..'):
            if file.endswith('.xlsx'):
                print(f" - {file}")
else:
    print(f"âœ… æ‰¾åˆ°æºæ–‡ä»¶: {excel_source_file}")

wb = openpyxl.load_workbook(excel_source_file)
sheet = wb.active

# åˆ›å»ºç»“æœExcelæ–‡ä»¶
def create_result_excel(filename=None):
    """åˆ›å»ºç»“æœExcelæ–‡ä»¶"""
    # å¦‚æœä¸ä¿å­˜åˆ°Excelï¼Œç›´æ¥è¿”å›None
    if not SAVE_TO_EXCEL:
        return None
    
    # ç”Ÿæˆæ–‡ä»¶åï¼Œç¡®ä¿ä½¿ç”¨å½“å‰ç›®å½•è·¯å¾„
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"news_results_{timestamp}.xlsx"
    
    # ç¡®ä¿æ–‡ä»¶ä¿å­˜åœ¨æ­£ç¡®çš„ç›®å½•ä¸‹
    if not os.path.isabs(filename):
        # ä½¿ç”¨å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        filename = os.path.join(current_dir, filename)
    
    # ä¿å­˜å…¨å±€å¼•ç”¨
    global result_excel_file
    result_excel_file = filename
    
    # åˆ›å»ºå·¥ä½œç°¿å’Œå·¥ä½œè¡¨
    wb = openpyxl.Workbook()
    sheet = wb.active
    sheet.title = "æŠ“å–ç»“æœ"
    
    # è®¾ç½®è¡¨å¤´
    headers = [
        'ç´¢å¼•', 'æ¥æº', 'é“¾æ¥', 'æ ‡é¢˜', 'å‘å¸ƒæ—¥æœŸ', 'åˆ†ç±»', 'æ­£æ–‡å­—æ•°', 
        'çˆ¬å–æ—¶é—´(ç§’)', 'çŠ¶æ€', 'å°è¯•æ¬¡æ•°', 'AIéªŒè¯åˆ†æ•°', 'AIéªŒè¯ç†ç”±', 'é“¾æ¥ç±»å‹'
    ]
    
    # è®¾ç½®åˆ—å®½
    column_widths = {
        'A': 8,   # ç´¢å¼•
        'B': 15,  # æ¥æº
        'C': 50,  # é“¾æ¥
        'D': 50,  # æ ‡é¢˜
        'E': 15,  # å‘å¸ƒæ—¥æœŸ
        'F': 15,  # åˆ†ç±»
        'G': 10,  # æ­£æ–‡å­—æ•°
        'H': 15,  # çˆ¬å–æ—¶é—´
        'I': 20,  # çŠ¶æ€
        'J': 10,  # å°è¯•æ¬¡æ•°
        'K': 12,  # AIéªŒè¯åˆ†æ•°
        'L': 40,  # AIéªŒè¯ç†ç”±
        'M': 15,  # é“¾æ¥ç±»å‹
    }
    
    for col, width in column_widths.items():
        sheet.column_dimensions[col].width = width
    
    # å†™å…¥è¡¨å¤´
    for idx, header in enumerate(headers, 1):
        sheet.cell(row=1, column=idx).value = header
    
    # ä¿å­˜å¹¶è¿”å›
    wb.save(filename)
    print(f"å·²åˆ›å»ºç»“æœæ–‡ä»¶: {filename}")
    return filename

# ç”Ÿæˆå½“å‰æ‰¹æ¬¡ID
current_batch = datetime.now().strftime("%Y%m%d_%H%M%S")
batch_id = current_batch

# å…¨å±€å˜é‡ - ä¿®æ”¹é»˜è®¤è·¯å¾„ä½¿ç”¨å½“å‰ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
result_excel_file = os.path.join(current_dir, f"news_results_{batch_id}.xlsx")  # é»˜è®¤Excelæ–‡ä»¶åä½¿ç”¨å®Œæ•´è·¯å¾„
LOG_FILE = os.path.join(current_dir, "ai_link_decisions.jsonl")  # AIéªŒè¯æ—¥å¿—æ–‡ä»¶è·¯å¾„
ENABLE_LOGGING = True  # æ˜¯å¦å¯ç”¨AIéªŒè¯æ—¥å¿—

# ç®€åŒ–è¯„ä¼°å†…å®¹è´¨é‡çš„å‡½æ•°
def evaluate_content_quality(html_content, title, url=""):
    """
    ç®€åŒ–ç‰ˆå‡½æ•°ï¼Œä¸å†è¯„ä¼°å†…å®¹è´¨é‡ï¼Œåªè¿”å›å†…å®¹æ‘˜è¦å’ŒæŒ‡çº¹
    """
    if not html_content:
        return True, "", ""
    
    # ä½¿ç”¨BeautifulSoupè§£æHTML
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ç§»é™¤è„šæœ¬ã€æ ·å¼å’Œå¯¼èˆªå…ƒç´ 
    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
        tag.decompose()
    
    # è·å–æ‰€æœ‰æ–‡æœ¬
    text = soup.get_text(separator='\n', strip=True)
    
    # ç”Ÿæˆå†…å®¹æ‘˜è¦ (é™åˆ¶ä¸º200å­—)
    content_summary = text[:200] + "..." if len(text) > 200 else text
    
    # ç”Ÿæˆå†…å®¹æŒ‡çº¹
    content_fingerprint = hashlib.md5(text[:1000].encode('utf-8')).hexdigest()
    
    # å§‹ç»ˆè¿”å›Trueè¡¨ç¤ºå†…å®¹æœ‰æ•ˆ
    return True, content_summary, content_fingerprint

# æå–å‘å¸ƒæ—¶é—´çš„å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰
def extract_publish_date(soup):
    """
    ä»HTMLä¸­æå–å‘å¸ƒæ—¶é—´ï¼Œå°è¯•å¤šç§æ–¹æ³•
    """
    # æ–¹æ³•1: æŸ¥æ‰¾å…ƒæ•°æ®æ ‡ç­¾
    meta_properties = [
        'article:published_time', 'datePublished', 'publishedDate', 
        'pubdate', 'date', 'DC.date', 'article:modified_time', 
        'lastModified', 'og:published_time', 'og:updated_time',
        'dateCreated', 'dateModified', 'release_date'
    ]
    
    for meta in soup.find_all('meta'):
        prop = meta.get('property', '') or meta.get('name', '')
        if prop.lower() in [p.lower() for p in meta_properties]:
            content = meta.get('content')
            if content:
                return content
    
    # æ–¹æ³•2: æŸ¥æ‰¾æ—¶é—´æ ‡ç­¾
    time_tags = soup.find_all('time')
    for time_tag in time_tags:
        datetime_attr = time_tag.get('datetime')
        if datetime_attr:
            return datetime_attr
        if time_tag.text.strip():
            return time_tag.text.strip()
    
    # æ–¹æ³•3: æ ¹æ®CSSé€‰æ‹©å™¨æŸ¥æ‰¾
    date_selectors = [
        '.date', '.time', '.published', '.timestamp', '.post-date',
        '[itemprop="datePublished"]', '.article-date', '.entry-date',
        '.post-meta time', '.article-meta time', '.publish-date',
        '.updated-date', '.create-date', '.release-date', '.article-time'
    ]
    
    for selector in date_selectors:
        date_element = soup.select_one(selector)
        if date_element:
            return re.sub(r'(Published|Updated|Posted|Date):?\s*', '', date_element.get_text(strip=True))
    
    # æ–¹æ³•4: åœ¨å…¨æ–‡ä¸­æŸ¥æ‰¾æ—¥æœŸæ¨¡å¼
    # å¸¸è§æ—¥æœŸæ ¼å¼ï¼šYYYY-MM-DD, DD/MM/YYYY, Month DD, YYYYç­‰
    date_patterns = [
        r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
        r'\d{2}/\d{2}/\d{4}',  # DD/MM/YYYY
        r'\d{2}\.\d{2}\.\d{4}', # DD.MM.YYYY
        r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s+\d{4}',
        r'\d{1,2}\s+(January|February|March|April|May|June|July|August|September|October|November|December),?\s+\d{4}'
    ]
    
    text = soup.get_text()
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    
    # æ²¡æœ‰æ‰¾åˆ°æ—¥æœŸ
    return "æœªçŸ¥"

# æå–æ–‡ç« æ ‡é¢˜çš„å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰
def extract_title(soup, url):
    """
    ä»HTMLä¸­æå–æ–‡ç« æ ‡é¢˜ï¼Œå°è¯•å¤šç§æ–¹æ³•
    """
    # æ–¹æ³•1: ä½¿ç”¨<title>æ ‡ç­¾
    if soup.title:
        title = soup.title.string
        # æ¸…ç†æ ‡é¢˜ï¼ˆç§»é™¤ç½‘ç«™åç§°ç­‰ï¼‰
        if title:
            # å¸¸è§åˆ†éš”ç¬¦ï¼š|, -, â€“, â€”, :, Â·, â€¢
            for separator in [' | ', ' - ', ' â€“ ', ' â€” ', ' : ', ' Â· ', ' â€¢ ']:
                if separator in title:
                    parts = title.split(separator)
                    # é€šå¸¸ç¬¬ä¸€éƒ¨åˆ†æ˜¯æ–‡ç« æ ‡é¢˜ï¼Œè€Œéç½‘ç«™åç§°
                    if len(parts[0]) > 10:  # æ ‡é¢˜è¶³å¤Ÿé•¿
                        return parts[0].strip()
            return title.strip()
    
    # æ–¹æ³•2: å¯»æ‰¾ä¸»æ ‡é¢˜æ ‡ç­¾
    for heading in ['h1', 'h2']:
        headings = soup.find_all(heading)
        if headings:
            # ç­›é€‰æœ€å¯èƒ½æ˜¯æ ‡é¢˜çš„å…ƒç´ ï¼ˆé€šå¸¸ä½äºå†…å®¹åŒºåŸŸï¼Œä¸åœ¨header/navç­‰ä¸­ï¼‰
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=lambda c: c and ('content' in c.lower() or 'article' in c.lower()))
            
            if main_content:
                content_headings = main_content.find_all(heading)
                if content_headings:
                    return content_headings[0].get_text(strip=True)
            
            # å¦‚æœä¸èƒ½ç¡®å®šå†…å®¹åŒºåŸŸï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæœ€é•¿çš„æ ‡é¢˜
            longest_heading = max(headings, key=lambda h: len(h.get_text(strip=True)))
            return longest_heading.get_text(strip=True)
    
    # æ–¹æ³•3: æŸ¥æ‰¾å…ƒæ•°æ®
    meta_tags = soup.find_all('meta')
    for meta in meta_tags:
        prop = meta.get('property', '') or meta.get('name', '')
        if prop.lower() in ['og:title', 'twitter:title', 'dc.title']:
            content = meta.get('content')
            if content:
                return content
    
    # æ–¹æ³•4: ä»URLä¸­æå–å¯èƒ½çš„æ ‡é¢˜
    path = urlparse(url).path
    segments = [seg for seg in path.split('/') if seg]
    if segments:
        # æœ€åä¸€æ®µè·¯å¾„å¯èƒ½æ˜¯æ ‡é¢˜çš„slug
        last_segment = segments[-1]
        # å°†è¿å­—ç¬¦æˆ–ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼
        title_from_url = re.sub(r'[-_]', ' ', last_segment)
        if len(title_from_url) > 5:  # ç¡®ä¿è¶³å¤Ÿé•¿
            return title_from_url.capitalize()
    
    # æ— æ³•ç¡®å®šæ ‡é¢˜
    return "æœªæ‰¾åˆ°æ ‡é¢˜"

# åˆ¤æ–­é“¾æ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ–°é—»é“¾æ¥çš„å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
def is_valid_news_link(link, base_url, a_tag=None, html_content=None):
    """
    åˆ¤æ–­é“¾æ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ–°é—»é“¾æ¥ - ç®€åŒ–ç‰ˆï¼Œä¸»è¦ä¾èµ–AI
    
    å‚æ•°:
        link: å¾…éªŒè¯çš„é“¾æ¥
        base_url: åŸºç¡€é“¾æ¥
        a_tag: å¯é€‰ï¼Œé“¾æ¥æ‰€åœ¨çš„aæ ‡ç­¾ï¼ˆBeautifulSoupå¯¹è±¡ï¼‰
        html_content: å¯é€‰ï¼Œé¡µé¢HTMLå†…å®¹
    """
    # åœ¨å†·å¯åŠ¨æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰é“¾æ¥éƒ½è§†ä¸ºæœ‰æ•ˆ
    if USE_COLD_START:
        return True
        
    # åŸºç¡€è¿‡æ»¤ - æ˜æ˜¾çš„éæ–°é—»é“¾æ¥
    if not link or not isinstance(link, str):
        return False
        
    if link.startswith(('#', 'javascript:', 'mailto:', 'tel:')):
        return False
        
    # æ„å»ºå®Œæ•´URL
    if not urlparse(link).netloc:
        full_url = urljoin(base_url, link)
        parsed_link = urlparse(full_url)
    else:
        full_url = link
        parsed_link = urlparse(full_url)
        
    # æ’é™¤éæ–‡æœ¬åª’ä½“æ–‡ä»¶
    if any(parsed_link.path.lower().endswith(ext) for ext in 
          ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.zip', '.mp3', '.mp4', 
           '.css', '.js', '.ico', '.svg', '.webp', '.woff', '.ttf']):
        return False
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºåŒä¸€åŸŸåï¼ˆæ’é™¤å¤–éƒ¨é“¾æ¥ï¼‰
    parsed_base = urlparse(base_url)
    if parsed_link.netloc != parsed_base.netloc:
        return False

    # ä¼˜å…ˆä½¿ç”¨AIéªŒè¯æ¨¡å—
    if USE_AI_LINK_VALIDATION and AI_LINK_VALIDATOR_AVAILABLE and not USE_COLD_START:
        try:
            # ä½¿ç”¨AIéªŒè¯æ¨¡å—åˆ¤æ–­é“¾æ¥æœ‰æ•ˆæ€§
            from ai_link_validator import is_valid_news_link_with_ai
            return is_valid_news_link_with_ai(full_url, base_url, a_tag, html_content)
        except Exception as e:
            print(f"âŒ AIé“¾æ¥éªŒè¯å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€è§„åˆ™: {str(e)}")
    
    # åŸºç¡€è¿‡æ»¤è§„åˆ™ï¼ˆå¦‚æœAIéªŒè¯ä¸å¯ç”¨æˆ–å¤±è´¥ï¼‰
    # æ£€æŸ¥é“¾æ¥æ–‡æœ¬æ˜¯å¦çœ‹èµ·æ¥åƒæ ‡é¢˜ï¼ˆè¶…è¿‡20ä¸ªå­—ç¬¦ä¸”ä¸æ˜¯å¯¼èˆªç±»æ–‡æœ¬ï¼‰
    if a_tag and a_tag.get_text():
        link_text = a_tag.get_text().strip()
        if len(link_text) > 15:  # æ”¾å®½æ¡ä»¶
            return True
    
    # æ£€æŸ¥URLè·¯å¾„æ˜¯å¦åŒ…å«æ–°é—»ç›¸å…³ç‰¹å¾
    path_lower = parsed_link.path.lower()
    return (
        '/news/' in path_lower or 
        '/article/' in path_lower or 
        '/post/' in path_lower or
        re.search(r'/20\d{2}[/-]\d{1,2}/', path_lower) is not None or
        re.search(r'/\d{4,}/', path_lower) is not None
    )

# çˆ¬å–ä¸»ç•Œé¢
async def fetch_news_links(main_url, source):
    """è·å–æ–°é—»é“¾æ¥ï¼Œä¼˜åŒ–é“¾æ¥æ¯”å¯¹è¿‡ç¨‹"""
    # åˆå§‹åŒ–ç»Ÿè®¡
    link_counts = {
        "found_links": 0,
        "new_links": 0,
        "valid_links": 0,
        "invalid_links": 0,
        "skipped_existing": 0,
        "skipped_invalid": 0,
        "skipped_processed": 0,
        "total_ai_validated": 0
    }
    
    try:
        print(f"ğŸ”„ æ­£åœ¨çˆ¬å–ä¸»é¡µ: {main_url}")
        
        # è®¾ç½®çˆ¬è™«é…ç½®
        prune_filter = PruningContentFilter(
            threshold=0.45,
            threshold_type="dynamic",
            min_word_threshold=5
        )
        md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)
        
        crawler_config = CrawlerRunConfig(
            markdown_generator=md_generator,
            page_timeout=REQUEST_TIMEOUT * 1000,
            cache_mode=CacheMode.BYPASS  # ç¦ç”¨ç¼“å­˜
        )
        
        # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•è°ƒç”¨çˆ¬è™«
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=main_url, config=crawler_config)
            
            if not result or not result.success or not result.html:
                return None, None, link_counts
                
            print(f"âœ… çˆ¬å–ä¸»é¡µæˆåŠŸ: {main_url}")
            
            # ä¼˜åŒ–ï¼šåŠ è½½å†å²é“¾æ¥æ•°æ®å¹¶é¢„å¤„ç†ä¸ºé«˜æ•ˆçš„æŸ¥æ‰¾ç»“æ„
            links_history = load_links_history(source)
            
            # ä½¿ç”¨é›†åˆå­˜å‚¨å·²çŸ¥URLï¼Œå®ç°O(1)æŸ¥æ‰¾
            known_urls_set = set()
            invalid_urls_set = set()
            processed_urls_set = set()
            
            # é¢„å¤„ç†é“¾æ¥ï¼Œåªéå†ä¸€æ¬¡å†å²è®°å½•
            for url, data in links_history.items():
                # å°†URLè§„èŒƒåŒ–å­˜å‚¨åœ¨é›†åˆä¸­
                norm_url = normalize_url(url)
                known_urls_set.add(norm_url)
                
                # åˆ†ç±»å­˜å‚¨åˆ°ä¸åŒçš„é›†åˆä¸­å®ç°O(1)æŸ¥æ‰¾
                if not data.get("is_valid", False):
                    invalid_urls_set.add(norm_url)
                elif data.get("content_length", 0) > 0:
                    processed_urls_set.add(norm_url)
                    
            print(f"âš ï¸ å·²åŠ è½½ {len(known_urls_set)} ä¸ªå·²çŸ¥é“¾æ¥ï¼Œå…¶ä¸­ {len(invalid_urls_set)} ä¸ªæ— æ•ˆé“¾æ¥")
            print(f"âš ï¸ å·²æœ‰ {len(processed_urls_set)} ä¸ªé“¾æ¥å·²ç»å¤„ç†è¿‡å†…å®¹")
            
            # ä¼˜åŒ–ï¼šè§£æHTMLå¹¶ä½¿ç”¨å­—å…¸å­˜å‚¨é“¾æ¥ï¼Œé¿å…é‡å¤å¤„ç†
            soup = BeautifulSoup(result.html, 'html.parser')
            a_tags = soup.find_all('a', href=True)
            
            # ä½¿ç”¨å­—å…¸å­˜å‚¨é“¾æ¥ï¼Œè‡ªåŠ¨å»é‡
            seen_links = {}
            # ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜å‡å°‘normalize_urlé‡å¤è°ƒç”¨
            normalize_cache = {}
            
            # ç¬¬ä¸€æ¬¡éå†ï¼šæ„å»ºè§„èŒƒåŒ–é“¾æ¥æ˜ å°„ï¼Œå®ç°æ›´é«˜æ•ˆçš„URLå¤„ç†
            for a in a_tags:
                href = a['href']
                # è§„èŒƒåŒ–é“¾æ¥ï¼Œç¡®ä¿å®Œæ•´URL
                if href.startswith('/'):
                    # ç›¸å¯¹URLï¼Œæ·»åŠ åŸŸå
                    parsed_url = urlparse(main_url)
                    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
                    url = urljoin(base_url, href)
                elif not href.startswith(('http://', 'https://')):
                    # å…¶ä»–ç›¸å¯¹URLå½¢å¼
                    url = urljoin(main_url, href)
                else:
                    url = href
                
                # ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è§„èŒƒåŒ–
                if url in normalize_cache:
                    norm_url = normalize_cache[url]
                else:
                    norm_url = normalize_url(url)
                    normalize_cache[url] = norm_url
                    
                if not norm_url:
                    continue
                    
                # ä½¿ç”¨è§„èŒƒåŒ–URLä½œä¸ºé”®ï¼Œé¿å…é‡å¤
                if norm_url not in seen_links:
                    seen_links[norm_url] = {
                        'url': url,  # ä¿ç•™åŸå§‹URLç”¨äºæ˜¾ç¤º
                        'a_tag': a,
                        'norm_url': norm_url  # å­˜å‚¨è§„èŒƒåŒ–URLç”¨äºæ¯”è¾ƒ
                    }
            
            print(f"ğŸ” æ‰¾åˆ° {len(seen_links)} ä¸ªåŸå§‹é“¾æ¥")
            
            # éœ€è¦AIéªŒè¯çš„é“¾æ¥
            links_to_validate = []
            
            # ç”¨äºå‚¨å­˜é“¾æ¥å’Œç›¸å…³ä¿¡æ¯
            links_info = []
            
            # ç»Ÿè®¡é‡å¤é“¾æ¥æ•°é‡
            duplicate_count = 0
            
            # æ‰¹é‡å¤„ç†é“¾æ¥ä»¥æé«˜æ•ˆç‡
            
            # 1. é¢„å…ˆæå–æ‰€æœ‰éœ€è¦å¤„ç†çš„é“¾æ¥
            links_to_process = []
            for norm_url, link_data in seen_links.items():
                # å¿«é€Ÿæ£€æŸ¥æ˜¯å¦å·²çŸ¥é“¾æ¥(ä½¿ç”¨O(1)å¤æ‚åº¦çš„é›†åˆæ“ä½œ)
                if norm_url in known_urls_set:
                    duplicate_count += 1
                    if norm_url in invalid_urls_set:
                        link_counts["skipped_invalid"] += 1
                    elif norm_url in processed_urls_set:
                        link_counts["skipped_processed"] += 1
                    else:
                        link_counts["skipped_existing"] += 1
                    continue
                
                # åªå¤„ç†æ–°é“¾æ¥
                links_to_process.append((norm_url, link_data))
            
            # 2. æ‰¹é‡å¤„ç†æ–°é“¾æ¥
            if links_to_process:
                print(f"ğŸ†• å‘ç° {len(links_to_process)} ä¸ªæ–°é“¾æ¥ï¼Œå¼€å§‹å¤„ç†...")
                
                # å†·å¯åŠ¨æ¨¡å¼ä¸æ™®é€šæ¨¡å¼çš„å¤„ç†é€»è¾‘åˆ†ç¦»ï¼Œé¿å…æ¯æ¬¡é“¾æ¥éƒ½åˆ¤æ–­æ¨¡å¼
                if USE_COLD_START:
                    # å†·å¯åŠ¨æ¨¡å¼ï¼šæ‰¹é‡å¤„ç†æ‰€æœ‰æ–°é“¾æ¥
                    batch_updates = {}
                    current_time = datetime.now().isoformat()
                    
                    for norm_url, link_data in links_to_process:
                        url = link_data['url']
                        a_tag = link_data['a_tag']
                        link_text = a_tag.get_text(strip=True) if a_tag else ""
                        
                        link_counts["found_links"] += 1
                        link_counts["new_links"] += 1
                        
                        # å°†é“¾æ¥åŠä¿¡æ¯ç›´æ¥æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                        links_info.append({
                            "url": url,
                            "a_tag": a_tag,
                            "title": link_text or url,
                            "is_valid": True,  # å†·å¯åŠ¨æ¨¡å¼ä¸‹ï¼Œé»˜è®¤è®¤ä¸ºæœ‰æ•ˆ
                            "ai_score": 0,
                            "ai_reason": "å†·å¯åŠ¨æ¨¡å¼ï¼šè·³è¿‡éªŒè¯"
                        })
                        
                        # å‡†å¤‡æ‰¹é‡æ›´æ–°æ•°æ®
                        batch_updates[url] = {
                            "url": url,
                            "title": link_text or url,
                            "first_seen": current_time,
                            "last_updated": current_time,
                            "is_valid": True,
                            "quality_score": 0,
                            "content_length": 0,
                            "error_message": "",
                            "content_fingerprint": "",
                            "ai_score": 0,
                            "ai_reason": "å†·å¯åŠ¨æ¨¡å¼ï¼šè·³è¿‡éªŒè¯",
                            "crawl_count": 0,
                            "link_type": ""
                        }
                    
                    # æ‰¹é‡æ›´æ–°å†å²è®°å½•
                    if batch_updates:
                        links_history.update(batch_updates)
                        save_links_history(source, links_history)
                        print(f"å†·å¯åŠ¨æ¨¡å¼ï¼šå·²æˆåŠŸå°† {len(batch_updates)} ä¸ªé“¾æ¥æ·»åŠ åˆ°å†å²è®°å½•ã€‚")
                    
                    return None, None, link_counts
                else:
                    # æ™®é€šæ¨¡å¼ï¼šæ‰¹é‡éªŒè¯è¿‡æ»¤åçš„é“¾æ¥
                    # 1. å…ˆç”¨åŸºæœ¬è§„åˆ™é¢„ç­›é€‰
                    for norm_url, link_data in links_to_process:
                        url = link_data['url']
                        a_tag = link_data['a_tag']
                        
                        # è®¡å…¥æ‰¾åˆ°çš„é“¾æ¥æ€»æ•°
                        link_counts["found_links"] += 1
                        link_counts["new_links"] += 1
                        
                        # ä½¿ç”¨åŸºç¡€è§„åˆ™è¿›è¡Œåˆç­›
                        is_valid_by_basic = is_valid_news_link(url, main_url, a_tag)
                        
                        # æ— è®ºæ˜¯å¦é€šè¿‡åŸºç¡€è§„åˆ™ï¼Œéƒ½è®°å½•æ‰€æœ‰æ–°é“¾æ¥åˆ°å†å²è®°å½•ä¸­
                        current_time = datetime.now().isoformat()
                        links_history[url] = {
                            "url": url,
                            "title": a_tag.get_text(strip=True) if a_tag else url,
                            "first_seen": current_time,
                            "last_updated": current_time,
                            "is_valid": is_valid_by_basic,  # åˆå§‹ç”¨åŸºæœ¬è§„åˆ™åˆ¤æ–­
                            "quality_score": 0,
                            "content_length": 0,
                            "error_message": "" if is_valid_by_basic else "æœªé€šè¿‡åŸºæœ¬è§„åˆ™æ£€æŸ¥",
                            "content_fingerprint": "",
                            "ai_score": 0,
                            "ai_reason": "",
                            "crawl_count": 0,
                            "link_type": ""
                        }
                        
                        # åªå°†é€šè¿‡åŸºç¡€è§„åˆ™çš„é“¾æ¥æ·»åŠ åˆ°å¾…éªŒè¯åˆ—è¡¨
                        if is_valid_by_basic:
                            links_to_validate.append({
                                "url": url,
                                "a_tag": a_tag,
                                "title": a_tag.get_text(strip=True) if a_tag else url
                            })
                    
                    # 2. æ‰¹é‡AIéªŒè¯
                    print(f"ğŸ”„ å¤„ç†åå¾—åˆ° {link_counts['found_links']} ä¸ªé“¾æ¥ï¼Œå…¶ä¸­ {len(links_to_validate)} ä¸ªéœ€è¦éªŒè¯")
                    print(f"â© è·³è¿‡äº† {duplicate_count} ä¸ªé‡å¤é“¾æ¥")
                    print(f"â© è·³è¿‡äº† {link_counts['skipped_existing']} ä¸ªå·²çŸ¥æœ‰æ•ˆé“¾æ¥")
                    print(f"â© è·³è¿‡äº† {link_counts['skipped_invalid']} ä¸ªå·²çŸ¥æ— æ•ˆé“¾æ¥")
                    print(f"â© è·³è¿‡äº† {link_counts['skipped_processed']} ä¸ªå·²å¤„ç†è¿‡å†…å®¹çš„é“¾æ¥")
                    
                    valid_links = []
                    if links_to_validate:
                        print(f"ğŸ§  ä½¿ç”¨AIéªŒè¯ {len(links_to_validate)} ä¸ªé“¾æ¥...")
                        
                        # æ›´æ–°è®¡æ•°
                        link_counts["total_ai_validated"] = len(links_to_validate)
                        
                        if USE_AI_LINK_VALIDATION and AI_LINK_VALIDATOR_AVAILABLE:
                            try:
                                # å¯¼å…¥batch_link_validationå‡½æ•°
                                from ai_link_validator import batch_link_validation
                                
                                # æ‰¹é‡éªŒè¯é“¾æ¥
                                valid_links = batch_link_validation(links_to_validate, main_url, result.html)
                                
                                # è°ƒè¯•æ‰“å°éªŒè¯ç»“æœ
                                print(f"âœ… AIéªŒè¯å®Œæˆï¼Œå¾—åˆ° {len(valid_links)} ä¸ªæœ‰æ•ˆé“¾æ¥")
                                
                                # æ›´æ–°ç»“æœç»Ÿè®¡
                                link_counts["valid_links"] = len(valid_links)
                                link_counts["invalid_links"] = len(links_to_validate) - len(valid_links)
                                
                                # å°†éªŒè¯ç»“æœæ·»åŠ åˆ°é“¾æ¥ä¿¡æ¯åˆ—è¡¨
                                for link_info in valid_links:
                                    link_info["is_valid"] = True
                                    links_info.append(link_info)
                                    
                                    # ç«‹å³å°†æœ‰æ•ˆé“¾æ¥å†™å…¥Excel
                                    write_valid_link_to_excel(
                                        url=link_info.get("url"),
                                        title=link_info.get("title", ""),
                                        source_name=source_name,
                                        ai_score=link_info.get("ai_score", 0),
                                        ai_reason=link_info.get("ai_reason", ""),
                                        link_type=link_info.get("link_type", "")
                                    )
                                
                                # ä¼˜åŒ–ï¼šæ‰¹é‡æ›´æ–°å†å²è®°å½• - æ›´é«˜æ•ˆçš„å®ç°
                                batch_updates = {}
                                current_time = datetime.now().isoformat()
                                validated_urls = {normalize_url(info.get("url")): info for info in valid_links}
                                
                                # ä¸€æ¬¡æ€§å‡†å¤‡æ‰€æœ‰æ›´æ–°æ•°æ® - ç¡®ä¿æ‰€æœ‰é“¾æ¥éƒ½è¢«æ›´æ–°åˆ°å†å²è®°å½•ä¸­
                                for link in links_to_validate:
                                    url = link.get("url")
                                    norm_url = normalize_url(url)
                                    
                                    # æ£€æŸ¥æ˜¯å¦é€šè¿‡éªŒè¯
                                    is_valid = norm_url in validated_urls
                                    link_info = validated_urls.get(norm_url, {})
                                    
                                    ai_score = link_info.get("ai_score", 0) if is_valid else 0
                                    ai_reason = link_info.get("ai_reason", "") if is_valid else "AIéªŒè¯æœªé€šè¿‡"
                                    
                                    # å‡†å¤‡æ›´æ–°æ•°æ® - æ— è®ºæ˜¯å¦æœ‰æ•ˆéƒ½ä¼šæ›´æ–°
                                    batch_updates[url] = {
                                        "url": url,
                                        "title": link.get("title", ""),
                                        "first_seen": current_time,
                                        "last_updated": current_time,
                                        "is_valid": is_valid,
                                        "quality_score": 0,
                                        "content_length": 0,
                                        "error_message": "" if is_valid else "AIéªŒè¯æœªé€šè¿‡",
                                        "content_fingerprint": "",
                                        "ai_score": ai_score,
                                        "ai_reason": ai_reason,
                                        "crawl_count": 0,
                                        "link_type": link_info.get("link_type", "")
                                    }
                                
                                # æ‰¹é‡æ›´æ–°å†å²è®°å½•
                                links_history.update(batch_updates)
                                save_links_history(source, links_history)
                                print(f"âœ… å·²æ›´æ–° {len(batch_updates)} ä¸ªé“¾æ¥åˆ°å†å²è®°å½•")
                                
                            except Exception as e:
                                print(f"âŒ AIæ‰¹é‡éªŒè¯å¤±è´¥: {str(e)}")
                                # å¯¹äºå¼‚å¸¸æƒ…å†µï¼Œä»ç„¶ç¡®ä¿é“¾æ¥è¢«æ·»åŠ åˆ°å†å²è®°å½•
                                batch_updates = {}
                                current_time = datetime.now().isoformat()
                                
                                for link in links_to_validate:
                                    url = link.get("url")
                                    batch_updates[url] = {
                                        "url": url,
                                        "title": link.get("title", ""),
                                        "first_seen": current_time,
                                        "last_updated": current_time,
                                        "is_valid": True,  # å‡ºé”™æ—¶é»˜è®¤ä¸ºæœ‰æ•ˆ
                                        "quality_score": 0,
                                        "content_length": 0,
                                        "error_message": f"AIéªŒè¯å¤±è´¥ï¼š{str(e)}",
                                        "content_fingerprint": "",
                                        "ai_score": 0,
                                        "ai_reason": f"AIéªŒè¯å¤±è´¥ï¼Œé»˜è®¤è§†ä¸ºæœ‰æ•ˆ: {str(e)}",
                                        "crawl_count": 0,
                                        "link_type": ""
                                    }
                                
                                links_history.update(batch_updates)
                                save_links_history(source, links_history)
                                print(f"âœ… å·²æ›´æ–° {len(batch_updates)} ä¸ªé“¾æ¥åˆ°å†å²è®°å½•ï¼ˆAIéªŒè¯å¤±è´¥ï¼‰")
                        else:
                            # ä¸ä½¿ç”¨AIéªŒè¯ï¼Œæ‰€æœ‰é€šè¿‡åŸºç¡€è§„åˆ™çš„é“¾æ¥éƒ½è§†ä¸ºæœ‰æ•ˆ
                            for link in links_to_validate:
                                link["is_valid"] = True
                                link["ai_score"] = 0
                                link["ai_reason"] = "è·³è¿‡AIéªŒè¯"
                                links_info.append(link)
                                valid_links.append(link)
                            
                            link_counts["valid_links"] = len(valid_links)
                    else:
                        print("âš ï¸ æ²¡æœ‰éœ€è¦éªŒè¯çš„æ–°é“¾æ¥")
                    
                    # åœ¨å¤„ç†ç»“æŸåå†æ¬¡ä¿å­˜å†å²è®°å½•ï¼ˆç¡®ä¿æ‰€æœ‰é“¾æ¥éƒ½è¢«è®°å½•ï¼‰
                    save_links_history(source, links_history)
                    
                    return valid_links, result.html, link_counts
            else:
                print("âš ï¸ æ²¡æœ‰å‘ç°æ–°é“¾æ¥")
                return [], result.html, link_counts
            
    except Exception as e:
        print(f"âŒ è·å–é“¾æ¥å¤±è´¥: {str(e)}")
        traceback.print_exc()
        return None, None, link_counts

# å¯¼å‡ºå†å²é“¾æ¥åº“åˆ°Excelï¼ˆæ–°å¢å‡½æ•°ï¼‰
def export_links_history_to_excel():
    """å°†æ‰€æœ‰å†å²é“¾æ¥æ•°æ®å¯¼å‡ºåˆ°Excelæ–‡ä»¶"""
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ç¡®ä¿æ–‡ä»¶ä¿å­˜åœ¨å½“å‰ç›®å½•ä¸‹
        current_dir = os.path.dirname(os.path.abspath(__file__))
        filename = os.path.join(current_dir, f"links_history_{timestamp}.xlsx")
        
        # åˆ›å»ºå·¥ä½œç°¿å’Œå·¥ä½œè¡¨
        wb = openpyxl.Workbook()
        sheet = wb.active
        sheet.title = "å†å²é“¾æ¥"
        
        # è®¾ç½®è¡¨å¤´
        headers = [
            'åºå·', 'é“¾æ¥ID', 'æ¥æº', 'URL', 'æ ‡é¢˜', 'æœ‰æ•ˆæ€§', 'é¦–æ¬¡å‘ç°æ—¶é—´', 
            'æœ€åæ›´æ–°æ—¶é—´', 'å†…å®¹é•¿åº¦', 'è´¨é‡è¯„åˆ†', 'çˆ¬å–æ¬¡æ•°', 'AIéªŒè¯åˆ†æ•°', 'AIéªŒè¯ç†ç”±', 'é“¾æ¥ç±»å‹'
        ]
        
        # è®¾ç½®åˆ—å®½
        column_widths = {
            'A': 8,   # åºå·
            'B': 35,  # é“¾æ¥ID
            'C': 15,  # æ¥æº
            'D': 60,  # URL
            'E': 50,  # æ ‡é¢˜
            'F': 10,  # æœ‰æ•ˆæ€§
            'G': 20,  # é¦–æ¬¡å‘ç°æ—¶é—´
            'H': 20,  # æœ€åæ›´æ–°æ—¶é—´
            'I': 12,  # å†…å®¹é•¿åº¦
            'J': 12,  # è´¨é‡è¯„åˆ†
            'K': 10,  # çˆ¬å–æ¬¡æ•°
            'L': 12,  # AIéªŒè¯åˆ†æ•°
            'M': 40,  # AIéªŒè¯ç†ç”±
            'N': 15,  # é“¾æ¥ç±»å‹
        }
        
        for col, width in column_widths.items():
            sheet.column_dimensions[col].width = width
        
        # å†™å…¥è¡¨å¤´
        for idx, header in enumerate(headers, 1):
            sheet.cell(row=1, column=idx).value = header
        
        # éå†æ‰€æœ‰å†å²è®°å½•æ–‡ä»¶
        row_idx = 2
        total_records = 0
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(LINKS_HISTORY_DIR):
            print(f"å†å²é“¾æ¥ç›®å½•ä¸å­˜åœ¨: {LINKS_HISTORY_DIR}")
            return None
            
        # è·å–æ‰€æœ‰å†å²è®°å½•æ–‡ä»¶
        history_files = [f for f in os.listdir(LINKS_HISTORY_DIR) if f.endswith('_links.json')]
        
        for history_file in history_files:
            # æå–æ¥æºID
            source = history_file.replace('_links.json', '')
            
            # åŠ è½½æ•°æ®
            file_path = os.path.join(LINKS_HISTORY_DIR, history_file)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    links_data = json.load(f)
                    
                # å†™å…¥æ•°æ®
                for link_id, info in links_data.items():
                    # æ„å»ºæ•°æ®è¡Œ
                    data = [
                        row_idx - 1,  # åºå·
                        link_id,      # é“¾æ¥ID
                        source,       # æ¥æº
                        info.get('url', ''),            # URL
                        info.get('title', ''),          # æ ‡é¢˜
                        'æœ‰æ•ˆ' if info.get('is_valid', False) else 'æ— æ•ˆ',  # æœ‰æ•ˆæ€§
                        info.get('first_seen', ''),     # é¦–æ¬¡å‘ç°æ—¶é—´
                        info.get('last_updated', ''),   # æœ€åæ›´æ–°æ—¶é—´
                        info.get('content_length', 0),  # å†…å®¹é•¿åº¦
                        info.get('quality_score', 0),   # è´¨é‡è¯„åˆ†
                        info.get('crawl_count', 0),     # çˆ¬å–æ¬¡æ•°
                        info.get('ai_score', 0),        # AIéªŒè¯åˆ†æ•°
                        info.get('ai_reason', ''),      # AIéªŒè¯ç†ç”±
                        info.get('link_type', '')       # é“¾æ¥ç±»å‹
                    ]
                    
                    # å†™å…¥ä¸€è¡Œæ•°æ®
                    for col, value in enumerate(data, 1):
                        sheet.cell(row=row_idx, column=col).value = value
                        
                    row_idx += 1
                    total_records += 1
                    
            except Exception as e:
                print(f"å¤„ç†å†å²æ–‡ä»¶æ—¶å‡ºé”™ {history_file}: {e}")
                continue
        
        # ä¿å­˜æ–‡ä»¶
        wb.save(filename)
        print(f"æˆåŠŸå¯¼å‡ºå†å²é“¾æ¥æ•°æ®åˆ° {filename}ï¼Œå…± {total_records} æ¡è®°å½•")
        return filename
        
    except Exception as e:
        print(f"å¯¼å‡ºå†å²é“¾æ¥æ•°æ®æ—¶å‡ºé”™: {e}")
        return None

# å°†æŠ“å–ç»“æœå†™å…¥Excelæ–‡ä»¶
def write_result_to_excel(url, title, source_name, publish_date, content, link_duration, 
                          is_valid_content, retry_count, processed_count, ai_score=0, 
                          ai_reason="", link_type=""):
    """å°†æŠ“å–ç»“æœå†™å…¥Excelæ–‡ä»¶"""
    if not SAVE_TO_EXCEL or not result_excel_file or not os.path.exists(result_excel_file):
        return
    
    try:
        # åŠ è½½Excelæ–‡ä»¶
        wb = openpyxl.load_workbook(result_excel_file)
        sheet = wb.active
        next_row = sheet.max_row + 1
        
        # æ„å»ºæ•°æ®
        data = [
            processed_count,  # ç´¢å¼•
            source_name,      # æ¥æº
            url,              # é“¾æ¥
            title,            # æ ‡é¢˜
            publish_date,     # å‘å¸ƒæ—¥æœŸ
            "æ–°é—»" if is_valid_content else "éæ–°é—»",  # åˆ†ç±»
            len(content),     # æ­£æ–‡å­—æ•°
            f"{link_duration:.2f}",    # çˆ¬å–æ—¶é—´
            "æˆåŠŸ" if is_valid_content else "å¤±è´¥",  # çŠ¶æ€
            retry_count,      # å°è¯•æ¬¡æ•°
            ai_score,         # AIéªŒè¯åˆ†æ•°
            ai_reason,        # AIéªŒè¯ç†ç”±
            link_type         # é“¾æ¥ç±»å‹
        ]
        
        # å†™å…¥æ•°æ®
        for col, value in enumerate(data, 1):
            sheet.cell(row=next_row, column=col).value = value
            
        # ä¿®æ”¹ï¼šç«‹å³ä¿å­˜Excelæ–‡ä»¶
        wb.save(result_excel_file)
        print(f"âœ… å·²å°†ç»“æœå†™å…¥Excel: è¡Œ {next_row} å¹¶ä¿å­˜")
    except Exception as e:
        print(f"âŒ å†™å…¥Excelæ—¶å‡ºé”™: {e}")

# ä¿®æ”¹å†™å…¥Excelå‡½æ•°ï¼Œç¡®ä¿å®æ—¶ä¿å­˜ï¼Œå¹¶åœ¨æ¯ä¸ªæœ‰æ•ˆé“¾æ¥è¢«éªŒè¯åç«‹å³å†™å…¥Excel
def write_valid_link_to_excel(url, title, source_name, ai_score=0, ai_reason="", link_type=""):
    """å°†æœ‰æ•ˆé“¾æ¥ä¿¡æ¯ç«‹å³å†™å…¥Excelæ–‡ä»¶"""
    if not SAVE_TO_EXCEL:
        return
    
    # ç¡®ä¿result_excel_fileæ˜¯å…¨å±€å˜é‡
    global result_excel_file
    
    # å¦‚æœç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå…ˆåˆ›å»º
    if not result_excel_file or not os.path.exists(result_excel_file):
        result_excel_file = create_result_excel()
        if not result_excel_file:
            print(f"âŒ æ— æ³•åˆ›å»ºExcelç»“æœæ–‡ä»¶")
            return
    
    try:
        # åŠ è½½Excelæ–‡ä»¶
        wb = openpyxl.load_workbook(result_excel_file)
        sheet = wb.active
        next_row = sheet.max_row + 1
        
        # æ„å»ºåŸºæœ¬æ•°æ®
        data = [
            next_row-1,     # ç´¢å¼•
            source_name,    # æ¥æº
            url,            # é“¾æ¥
            title,          # æ ‡é¢˜
            "å¾…å¤„ç†",       # å‘å¸ƒæ—¥æœŸï¼ˆå¾…çˆ¬å–å†…å®¹åæ›´æ–°ï¼‰
            "æ–°é—»",         # åˆ†ç±»
            0,              # æ­£æ–‡å­—æ•°ï¼ˆå¾…çˆ¬å–å†…å®¹åæ›´æ–°ï¼‰
            0,              # çˆ¬å–æ—¶é—´ï¼ˆå¾…çˆ¬å–å†…å®¹åæ›´æ–°ï¼‰
            "å¾…å¤„ç†",       # çŠ¶æ€
            0,              # å°è¯•æ¬¡æ•°
            ai_score,       # AIéªŒè¯åˆ†æ•°
            ai_reason,      # AIéªŒè¯ç†ç”±
            link_type       # é“¾æ¥ç±»å‹
        ]
        
        # å†™å…¥æ•°æ®
        for col, value in enumerate(data, 1):
            sheet.cell(row=next_row, column=col).value = value
        
        # ç«‹å³ä¿å­˜Excelæ–‡ä»¶
        wb.save(result_excel_file)
        print(f"âœ… å·²å°†æœ‰æ•ˆé“¾æ¥å†™å…¥Excel: è¡Œ {next_row} å¹¶ä¿å­˜")
    except Exception as e:
        print(f"âŒ å†™å…¥Excelæ—¶å‡ºé”™: {e}")
        traceback.print_exc()

# ä¸»ç¨‹åº
async def main():
    """ä¸»å‡½æ•°å…¥å£"""
    # åˆ›å»ºç»“æœExcelæ–‡ä»¶
    if SAVE_TO_EXCEL:
        result_file = create_result_excel()
        print(f"ğŸ“Š Excelç»“æœå°†ä¿å­˜åˆ°: {result_file}")
    
    # è®¾ç½®æ‰¹æ¬¡IDï¼ˆç”¨äºæ–‡ä»¶åå’Œæ—¥å¿—ï¼‰
    batch_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    print(f"ğŸš€ å¼€å§‹æ–°é—»çˆ¬å–ï¼Œæ‰¹æ¬¡ID: {batch_id}")
    
    # è¯»å–excelæ–‡ä»¶ä¸­çš„ä¸»é¡µé“¾æ¥ - ä½¿ç”¨å½“å‰è„šæœ¬ç›®å½•ä¸‹çš„æ–‡ä»¶
    current_dir = os.path.dirname(os.path.abspath(__file__))
    excel_source_file = os.path.join(current_dir, "testhomepage.xlsx")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(excel_source_file):
        print(f"âš ï¸ æºæ–‡ä»¶ä¸å­˜åœ¨: {excel_source_file}")
        excel_source_file = "../testhomepage.xlsx"
        if not os.path.exists(excel_source_file):
            print(f"âš ï¸ å¤‡ç”¨æºæ–‡ä»¶ä¹Ÿä¸å­˜åœ¨: {excel_source_file}")
            # å°è¯•åˆ—å‡ºå½“å‰ç›®å½•å’Œä¸Šçº§ç›®å½•ä¸­çš„Excelæ–‡ä»¶
            print("å½“å‰ç›®å½•ä¸‹çš„Excelæ–‡ä»¶:")
            for file in os.listdir('.'):
                if file.endswith('.xlsx'):
                    print(f" - {file}")
            
            print("ä¸Šçº§ç›®å½•ä¸‹çš„Excelæ–‡ä»¶:")
            for file in os.listdir('..'):
                if file.endswith('.xlsx'):
                    print(f" - {file}")
            return
    
    print(f"ğŸ“„ æ‰¾åˆ°æºæ–‡ä»¶: {excel_source_file}")
    wb_source = openpyxl.load_workbook(excel_source_file)
    sheet_source = wb_source.active
    
    # åˆ›å»ºå†å²é“¾æ¥ç›®å½•
    os.makedirs(LINKS_HISTORY_DIR, exist_ok=True)
    
    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
    source_count = 0         # å¤„ç†çš„æ¥æºæ•°é‡
    processed_count = 0      # å¤„ç†çš„é“¾æ¥æ•°é‡
    success_count = 0        # æˆåŠŸå¤„ç†çš„é“¾æ¥æ•°é‡
    error_count = 0          # å¤„ç†å¤±è´¥çš„é“¾æ¥æ•°é‡
    skipped_count = 0        # è·³è¿‡çš„é“¾æ¥æ•°é‡
    new_link_count = 0       # æ–°é“¾æ¥æ•°é‡
    invalid_link_count = 0   # æ— æ•ˆé“¾æ¥æ•°é‡
    
    # éå†excelæ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œ
    for row in range(2, sheet_source.max_row + 1):  # ä»ç¬¬2è¡Œå¼€å§‹ï¼Œè·³è¿‡æ ‡é¢˜è¡Œ
        try:
            remark = sheet_source.cell(row=row, column=1).value
            main_url = sheet_source.cell(row=row, column=2).value
            source_name = sheet_source.cell(row=row, column=3).value
            
            if not main_url or not source_name:
                continue
                
            source_count += 1
            source_success_count = 0  # åˆå§‹åŒ–æ¯ä¸ªæ¥æºçš„æˆåŠŸè®¡æ•°å™¨
            
            # æ¯ä¸ªæºçš„ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸŒ å¤„ç†æ¥æº {source_count}/{source_count}: {source_name} - {main_url} (Source ID: {source_name})")
            
            # åŠ è½½æ­¤æºçš„å†å²é“¾æ¥
            links_history_file = os.path.join(LINKS_HISTORY_DIR, f"{source_name}_links.json")
            print(f"ğŸ“‹ å†å²é“¾æ¥æ–‡ä»¶: {links_history_file}")
            
            # çˆ¬å–ä¸»é¡µä¸­çš„æ–°é—»é“¾æ¥
            start_time = time.time()
            links, html_content, stats = await fetch_news_links(main_url, source_name)
            end_time = time.time()
            
            print(f"é“¾æ¥çˆ¬å–æ—¶é—´: {end_time - start_time:.2f} ç§’")
            print(f"æ‰¾åˆ° {len(links) if links else 0} ä¸ªé“¾æ¥éœ€è¦éªŒè¯")
            
            # æ‰“å°é“¾æ¥è¿‡æ»¤è¯¦æƒ…
            print("é“¾æ¥è¿‡æ»¤è¯¦æƒ…:")
            print(f"- åŸå§‹é“¾æ¥æ€»æ•°: {stats['found_links']}")
            print(f"- è·³è¿‡é‡å¤é“¾æ¥: {stats['skipped_existing']}")
            print(f"- è·³è¿‡å·²çŸ¥æ— æ•ˆé“¾æ¥: {stats['skipped_invalid']}")
            print(f"- è·³è¿‡äº† {stats['skipped_processed']} ä¸ªå·²å¤„ç†è¿‡å†…å®¹çš„é“¾æ¥")
            
            # å¦‚æœæ˜¯å†·å¯åŠ¨æ¨¡å¼ï¼Œç›´æ¥è¿”å›ç»“æœï¼Œä¸è¿›è¡Œå®é™…å†…å®¹çˆ¬å–
            if USE_COLD_START:
                print("å†·å¯åŠ¨æ¨¡å¼ï¼šè·³è¿‡å†…å®¹çˆ¬å–æ­¥éª¤")
                # åœ¨å¾ªç¯ä¸­ä¸è¦ç›´æ¥è¿”å›ï¼Œè€Œæ˜¯è®°å½•ç»“æœå¹¶ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ¥æº
                # return {
                #     'source': source_name,
                #     'url': main_url,
                #     'valid_links': len(all_processed_links),
                #     'message': f"å†·å¯åŠ¨æ¨¡å¼ï¼šå·²å°† {len(all_processed_links)} ä¸ªé“¾æ¥æ·»åŠ åˆ°å†å²è®°å½•"
                # }
                # ç›´æ¥ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ¥æº
                continue
            
            # è¿›è¡ŒAIéªŒè¯
            valid_links = []
            
            if USE_AI_LINK_VALIDATION and AI_LINK_VALIDATOR_AVAILABLE and links:
                # ç¡®ä¿é“¾æ¥å¯¹è±¡ç»“æ„ä¸€è‡´
                links_for_validation = [
                    {'url': item.get('url', item.get('link')), 'title': item.get('title', '')}
                    for item in links
                ]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„é“¾æ¥éœ€è¦éªŒè¯
                if links_for_validation:
                    # ä½¿ç”¨AIéªŒè¯é“¾æ¥ï¼Œè¿™é‡Œçš„htmlå†…å®¹å¯ä»¥ä¸ºç©º
                    crawler_html = ""  # æˆ‘ä»¬ä¸å†ä¾èµ–resultå˜é‡
                    try:
                        from ai_link_validator import batch_link_validation
                        validated_links = batch_link_validation(
                            links_for_validation, 
                            main_url, 
                            crawler_html
                        )
                        
                        # å°†éªŒè¯åçš„é“¾æ¥ä¸åŸå§‹é“¾æ¥åˆå¹¶ï¼Œä¿æŒåŸå§‹çš„link_objç»“æ„
                        # åˆ›å»ºä¸€ä¸ªURLåˆ°éªŒè¯ç»“æœçš„æ˜ å°„
                        validation_results = {}
                        for validated_link in validated_links:
                            validation_results[validated_link.get('url')] = validated_link
                        
                        # æ›´æ–°åŸå§‹é“¾æ¥çš„éªŒè¯ä¿¡æ¯
                        for i, original_link in enumerate(links):
                            # ç¡®å®šURLé”®
                            url_key = 'url' if 'url' in original_link else 'link'
                            link_url = original_link.get(url_key)
                            
                            if link_url in validation_results:
                                validated_info = validation_results[link_url]
                                # æ›´æ–°éªŒè¯ä¿¡æ¯
                                original_link['is_valid'] = validated_info.get('is_valid', False)
                                original_link['ai_score'] = validated_info.get('ai_score', 0)
                                original_link['ai_reason'] = validated_info.get('ai_reason', '')
                                original_link['link_type'] = validated_info.get('link_type', '')
                                
                                # å¦‚æœéªŒè¯ç»“æœæ˜¯æœ‰æ•ˆçš„ï¼Œæ·»åŠ åˆ°valid_links
                                if validated_info.get('is_valid', False):
                                    # å°†æœ‰æ•ˆé“¾æ¥æ·»åŠ åˆ°valid_links
                                    valid_links.append(original_link)
                                    
                                    # ç«‹å³å°†æœ‰æ•ˆé“¾æ¥å†™å…¥Excel
                                    write_valid_link_to_excel(
                                        url=link_url,
                                        title=original_link.get('title', ''),
                                        source_name=source_name,
                                        ai_score=validated_info.get('ai_score', 0),
                                        ai_reason=validated_info.get('ai_reason', ''),
                                        link_type=validated_info.get('link_type', '')
                                    )
                    except Exception as e:
                        print(f"âŒ AIéªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
                        print("âš ï¸ è·³è¿‡AIéªŒè¯ï¼Œæ‰€æœ‰é“¾æ¥å°†è¢«è§†ä¸ºæœ‰æ•ˆ")
                        # é»˜è®¤æ‰€æœ‰é“¾æ¥æœ‰æ•ˆ
                        valid_links = links
                        
                        # ç¡®ä¿å³ä½¿å‡ºé”™ä¹Ÿå°†é“¾æ¥å†™å…¥Excel
                        for link in links:
                            url_key = 'url' if 'url' in link else 'link'
                            url = link.get(url_key)
                            title = link.get('title', '')
                            
                            # ç«‹å³å°†é“¾æ¥å†™å…¥Excel
                            write_valid_link_to_excel(
                                url=url,
                                title=title,
                                source_name=source_name,
                                ai_score=0,
                                ai_reason="AIéªŒè¯å¤±è´¥ï¼Œé»˜è®¤è§†ä¸ºæœ‰æ•ˆ",
                                link_type=""
                            )
                else:
                    print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆé“¾æ¥éœ€è¦éªŒè¯")
            else:
                # æ²¡æœ‰ä½¿ç”¨AIéªŒè¯æ—¶ï¼Œæ‰€æœ‰é“¾æ¥éƒ½è§†ä¸ºæœ‰æ•ˆ
                valid_links = links
                
                # ç¡®ä¿æ‰€æœ‰é“¾æ¥éƒ½è¢«å†™å…¥Excel
                for link in links:
                    url_key = 'url' if 'url' in link else 'link'
                    url = link.get(url_key)
                    title = link.get('title', '')
                    
                    # ç«‹å³å°†é“¾æ¥å†™å…¥Excel
                    write_valid_link_to_excel(
                        url=url,
                        title=title,
                        source_name=source_name,
                        ai_score=0,
                        ai_reason="è·³è¿‡AIéªŒè¯",
                        link_type=""
                    )
            
            # æ›´æ–°é“¾æ¥åˆ—è¡¨ä¸ºéªŒè¯åçš„æœ‰æ•ˆé“¾æ¥
            links = valid_links
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_content_time = time.time()
            
            # å¤„ç†æ¯ä¸ªé“¾æ¥çš„å†…å®¹
            for link_obj in links:
                # å¥å£®æ€§å¤„ç†ï¼šç¡®ä¿link_objå…·æœ‰url
                if isinstance(link_obj, dict):
                    # æœ‰äº›link_objå¯èƒ½ä½¿ç”¨'url'é”®ï¼Œæœ‰äº›å¯èƒ½ä½¿ç”¨'link'é”®
                    link = link_obj.get('url')
                    if link is None:
                        link = link_obj.get('link')
                    
                    title = link_obj.get('title', link if link else '')
                else:
                    # å¦‚æœlink_objä¸æ˜¯å­—å…¸ç±»å‹
                    print(f"âš ï¸ æœªçŸ¥çš„é“¾æ¥å¯¹è±¡ç±»å‹: {type(link_obj)}ï¼Œè·³è¿‡å¤„ç†")
                    continue
                
                if not link:
                    print("âš ï¸ é“¾æ¥ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                    continue
                
                processed_count += 1
                print(f"\nå¤„ç†é“¾æ¥ {processed_count}/{len(links)}: {link}")
                
                # æ£€æŸ¥é“¾æ¥æ˜¯å¦å·²ç»è¢«å¤„ç†è¿‡
                is_new, is_invalid, is_processed = is_new_link(link, source_name)
                
                if not is_new and is_processed:
                    print(f"è·³è¿‡å·²å¤„ç†è¿‡çš„é“¾æ¥: {link}")
                    continue
                
                # è®°å½•æ˜¯å¦ä¸ºæ–°é“¾æ¥
                if is_new:
                    new_link_count += 1
                
                retry_count = 0
                
                # åªæœ‰åˆ¤å®šä¸ºæœ‰æ•ˆé“¾æ¥æ‰è¿›è¡Œå†…å®¹çˆ¬å–
                if link_obj.get('is_valid', True):
                    while retry_count <= MAX_RETRY_COUNT:
                        try:
                            is_new, _, is_processed = is_new_link(link, source_name)
                            
                            if not is_new and is_processed:
                                print(f"è·³è¿‡å·²å¤„ç†è¿‡çš„é“¾æ¥: {link}")
                                break
                                
                            crawl_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            
                            print(f"\nğŸ”— æ­£åœ¨å¤„ç†é“¾æ¥: {link}")
                            print(f"ğŸ“Œ é“¾æ¥æ ‡é¢˜: {title}")
                            print(f"ğŸ†• æ˜¯å¦æ–°é“¾æ¥: {'æ˜¯' if is_new else 'å¦'}")
                            
                            # è®¾ç½®æµè§ˆå™¨é…ç½®
                            browser_config = BrowserConfig(
                                browser_type="chromium",
                                headless=True, 
                                viewport_width=1366,
                                viewport_height=768,
                                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
                                java_script_enabled=True,
                                ignore_https_errors=True
                            )
                            
                            # è®¾ç½®çˆ¬è™«é…ç½®
                            prune_filter = PruningContentFilter(
                                threshold=0.45,
                                threshold_type="dynamic",
                                min_word_threshold=5
                            )
                            md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)
                            
                            crawler_config = CrawlerRunConfig(
                                markdown_generator=md_generator,
                                page_timeout=REQUEST_TIMEOUT * 1000,
                                cache_mode=CacheMode.BYPASS
                            )
                            
                            # çˆ¬å–å†…å®¹
                            link_start_time = time.time()
                            async with AsyncWebCrawler(config=browser_config) as crawler:
                                result = await crawler.arun(url=link, config=crawler_config)
                            link_end_time = time.time()
                            
                            # è®¡ç®—çˆ¬å–è€—æ—¶
                            link_duration = link_end_time - link_start_time
                            
                            if result.success:
                                print(f"âœ… æˆåŠŸè·å–å†…å®¹, è€—æ—¶: {link_duration:.2f}ç§’")
                                
                                # ä½¿ç”¨BeautifulSoupå¤„ç†HTML
                                soup = BeautifulSoup(result.html, 'html.parser')
                                title = extract_title(soup, link) or title
                                publish_date = extract_publish_date(soup) or "æœªæ‰¾åˆ°æ—¥æœŸ"
                                content = result.markdown.fit_markdown
                                content_length = len(content)
                                
                                print(f"ğŸ“ æ ‡é¢˜: {title}")
                                print(f"ğŸ“… å‘å¸ƒæ—¥æœŸ: {publish_date}")
                                print(f"ğŸ“Š å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")
                                
                                # ä½¿ç”¨evaluate_content_qualityè·å–æ‘˜è¦å’ŒæŒ‡çº¹ï¼ˆä¸å†è¯„ä¼°å†…å®¹è´¨é‡ï¼‰
                                _, content_summary, content_fingerprint = evaluate_content_quality(result.html, title, link)
                                
                                # AIéªŒè¯é€šè¿‡çš„é“¾æ¥éƒ½è§†ä¸ºæœ‰æ•ˆ
                                is_valid_content = True
                                
                                # è¾“å‡ºç»“æœ
                                print(f"âœ… æˆåŠŸè·å–å†…å®¹, è€—æ—¶: {link_duration:.2f}ç§’")
                                print(f"ğŸ’¡ å†…å®¹æ‘˜è¦:\n{content_summary}")
                                
                                # çˆ¬å–å†…å®¹åï¼Œæ›´æ–°é“¾æ¥å†å²è®°å½•ï¼Œè®¾ç½®å†…å®¹é•¿åº¦ä»¥æ ‡è®°ä¸ºå·²å¤„ç†
                                link_obj['is_processed'] = True
                                
                                # è·å–AIéªŒè¯ä¿¡æ¯
                                ai_score = link_obj.get('ai_score', 0)
                                ai_reason = link_obj.get('ai_reason', "")
                                link_type = link_obj.get('link_type', "")
                                
                                # æ›´æ–°é“¾æ¥å†å²
                                update_link_history(
                                    url=link,
                                    title=title,
                                    source=source_name,
                                    content_summary=content_summary,
                                    is_valid=is_valid_content,
                                    quality_score=ai_score,
                                    content_length=content_length,
                                    content_fingerprint=content_fingerprint,
                                    ai_score=ai_score,
                                    ai_reason=ai_reason,
                                    link_type=link_type
                                )
                                
                                # å†™å…¥Excel
                                if SAVE_TO_EXCEL:
                                    write_result_to_excel(
                                        url=link,
                                        title=title,
                                        source_name=source_name,
                                        publish_date=publish_date,
                                        content=content,
                                        link_duration=link_duration,
                                        is_valid_content=is_valid_content,
                                        retry_count=retry_count,
                                        processed_count=processed_count,
                                        ai_score=ai_score,
                                        ai_reason=ai_reason,
                                        link_type=link_type
                                    )
                                
                                source_success_count += 1
                                success_count += 1
                                break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                                
                            else:
                                print(f"âŒ è·å–å†…å®¹å¤±è´¥: {result.error_message}")
                                error_message = result.error_message
                                
                                # æ›´æ–°é“¾æ¥å†å²è®°å½• - æ ‡è®°ä¸ºè·å–å¤±è´¥
                                update_link_history(
                                    url=link,
                                    title=title,
                                    source=source_name,
                                    is_valid=False,
                                    error_message=f"çˆ¬å–å¤±è´¥: {error_message}"
                                )
                                
                                retry_count += 1
                                if retry_count <= MAX_RETRY_COUNT:
                                    print(f"â³ é‡è¯• ({retry_count}/{MAX_RETRY_COUNT})...")
                                else:
                                    print(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒæ­¤é“¾æ¥")
                                    error_count += 1
                                
                        except Exception as e:
                            print(f"âŒ å¤„ç†é“¾æ¥æ—¶å‡ºé”™: {e}")
                            traceback.print_exc()
                            
                            # æ›´æ–°é“¾æ¥å†å²è®°å½• - æ ‡è®°ä¸ºå¤„ç†å¼‚å¸¸
                            update_link_history(
                                url=link,
                                title=title,
                                source=source_name,
                                is_valid=False,
                                error_message=f"å¤„ç†å¼‚å¸¸: {str(e)}"
                            )
                            
                            retry_count += 1
                            if retry_count <= MAX_RETRY_COUNT:
                                print(f"â³ é‡è¯• ({retry_count}/{MAX_RETRY_COUNT})...")
                            else:
                                print(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒæ­¤é“¾æ¥")
                                error_count += 1
                
                # å®Œæˆå½“å‰æ¥æºçš„å¤„ç†
                print(f"\nâœ… æ¥æº {source_name} å¤„ç†å®Œæˆï¼ŒæˆåŠŸè·å– {source_success_count} ä¸ªæœ‰æ•ˆé“¾æ¥")
                
        except Exception as e:
            print(f"âŒ å¤„ç†æº {source_name} æ—¶å‡ºé”™: {str(e)}")
            traceback.print_exc()
            continue
            
    # è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯æ‰“å°
    print(f"\nğŸ“Š çˆ¬å–ç»Ÿè®¡ä¿¡æ¯:")
    print(f"æ€»å…±å¤„ç†äº† {source_count} ä¸ªæ¥æº")
    print(f"æ€»å…±å‘ç°äº† {processed_count + skipped_count} ä¸ªé“¾æ¥")
    print(f" - å…¶ä¸­ {new_link_count} ä¸ªæ˜¯æ–°é“¾æ¥")
    print(f" - å…¶ä¸­ {skipped_count} ä¸ªé“¾æ¥è¢«è·³è¿‡ï¼ˆå·²å¤„ç†è¿‡æˆ–å·²çŸ¥æ— æ•ˆï¼‰")
    print(f"å®é™…å¤„ç†äº† {processed_count} ä¸ªé“¾æ¥")
    print(f" - å…¶ä¸­ {success_count} ä¸ªæœ‰æ•ˆå¹¶æˆåŠŸçˆ¬å–")
    print(f" - å…¶ä¸­ {error_count} ä¸ªå¤„ç†å¤±è´¥")
    print(f" - å…¶ä¸­ {invalid_link_count} ä¸ªè¢«AIåˆ¤æ–­ä¸ºæ— æ•ˆ")
    
    # è®¡ç®—æœ‰æ•ˆç‡
    if processed_count > 0:
        success_rate = (success_count / processed_count) * 100
        print(f"æœ‰æ•ˆç‡: {success_rate:.2f}%")
    
    # å¯¼å‡ºå†å²é“¾æ¥åˆ°Excel
    history_file = export_links_history_to_excel()
    if history_file:
        print(f"ğŸ“Š å†å²é“¾æ¥å·²å¯¼å‡ºåˆ°: {history_file}")
    
    print(f"ğŸ ç¨‹åºç»“æŸï¼Œæ‰¹æ¬¡ID: {batch_id}")

# å½“ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ—¶æ‰§è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == "--debug":
        # è°ƒè¯•æ¨¡å¼ï¼šçˆ¬å–å•ä¸ªé“¾æ¥
        if len(sys.argv) >= 4:
            # æ ¼å¼: python test_request3.py --debug <url> <source_name>
            debug_url = sys.argv[2]
            debug_source = sys.argv[3]
            
            print(f"ğŸ” è°ƒè¯•æ¨¡å¼: çˆ¬å–å•ä¸ªé“¾æ¥")
            print(f"ğŸ”— URL: {debug_url}")
            print(f"ğŸ“Œ æ¥æº: {debug_source}")
            
            # åˆ›å»ºç»“æœExcelæ–‡ä»¶
            if SAVE_TO_EXCEL:
                result_file = create_result_excel()
                print(f"ğŸ“Š Excelç»“æœå°†ä¿å­˜åˆ°: {result_file}")
            
            # ç¡®ä¿å†å²é“¾æ¥ç›®å½•å­˜åœ¨
            os.makedirs(LINKS_HISTORY_DIR, exist_ok=True)
            
            # å®šä¹‰å•ä¸ªé“¾æ¥è°ƒè¯•å‡½æ•°
            async def debug_single_link():
                try:
                    # åˆ›å»ºçˆ¬è™«é…ç½®
                    browser_config = BrowserConfig(
                        browser_type="chromium",
                        viewport_width=1280, 
                        viewport_height=800,
                        java_script_enabled=True,
                        ignore_https_errors=True
                    )
                    
                    # è®¾ç½®è¿‡æ»¤å™¨
                    prune_filter = PruningContentFilter(
                        threshold=0.45,
                        threshold_type="dynamic",
                        min_word_threshold=5
                    )
                    
                    # è®¾ç½®Markdownç”Ÿæˆå™¨
                    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)
                    
                    # çˆ¬è™«é…ç½®
                    crawler_config = CrawlerRunConfig(
                        markdown_generator=md_generator,
                        page_timeout=REQUEST_TIMEOUT * 1000,
                        cache_mode=CacheMode.BYPASS
                    )
                    
                    # å…ˆéªŒè¯é“¾æ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ–°é—»é“¾æ¥
                    print(f"ğŸ§  ä½¿ç”¨AIéªŒè¯é“¾æ¥æœ‰æ•ˆæ€§...")
                    parsed = urlparse(debug_url)
                    base_url = f"{parsed.scheme}://{parsed.netloc}"
                    
                    if AI_LINK_VALIDATOR_AVAILABLE:
                        # ä½¿ç”¨AIéªŒè¯
                        is_valid = is_valid_news_link_with_ai(debug_url, base_url)
                        if not is_valid:
                            print(f"âŒ AIåˆ¤æ–­è¯¥é“¾æ¥ä¸æ˜¯æœ‰æ•ˆçš„æ–°é—»é“¾æ¥ï¼Œä½†ä»å°†å°è¯•çˆ¬å–å†…å®¹")
                    else:
                        print(f"âš ï¸ AIéªŒè¯æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ç›´æ¥çˆ¬å–å†…å®¹")
                    
                    print(f"ğŸ”„ å¼€å§‹çˆ¬å–é“¾æ¥: {debug_url}")
                    start_time = time.time()
                    
                    # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶çˆ¬å–å†…å®¹
                    async with AsyncWebCrawler() as crawler:
                        result = await crawler.arun(url=debug_url, config=crawler_config)
                    
                    end_time = time.time()
                    duration = end_time - start_time
                    
                    if result.success:
                        # è§£æå†…å®¹
                        soup = BeautifulSoup(result.html, 'html.parser')
                        title = extract_title(soup, debug_url)
                        publish_date = extract_publish_date(soup) or "æœªæ‰¾åˆ°æ—¥æœŸ"
                        content = result.markdown.fit_markdown if hasattr(result.markdown, 'fit_markdown') else ""
                        content_length = len(content)
                        
                        # ä½¿ç”¨evaluate_content_qualityè·å–æ‘˜è¦å’ŒæŒ‡çº¹ï¼ˆä¸å†è¯„ä¼°å†…å®¹è´¨é‡ï¼‰
                        _, content_summary, content_fingerprint = evaluate_content_quality(result.html, title, debug_url)
                        
                        # è·å–AIéªŒè¯ä¿¡æ¯
                        ai_score = 0
                        ai_reason = ""
                        link_type = ""
                        
                        # ä»AIéªŒè¯æ—¥å¿—ä¸­æŸ¥æ‰¾æ­¤é“¾æ¥çš„è®°å½•
                        if ENABLE_LOGGING and os.path.exists(LOG_FILE):
                            try:
                                with open(LOG_FILE, "r", encoding="utf-8") as f:
                                    for line in f:
                                        try:
                                            record = json.loads(line)
                                            if record.get('url') == debug_url:
                                                ai_score = record.get('score', 0)
                                                ai_reason = record.get('reason', "")
                                                link_type = "æ–‡ç« " if record.get('is_valid', False) else "éæ–‡ç« "
                                                break
                                        except json.JSONDecodeError:
                                            continue
                            except Exception as e:
                                print(f"è¯»å–AIéªŒè¯æ—¥å¿—æ—¶å‡ºé”™: {e}")
                        
                        # è¾“å‡ºç»“æœ
                        print(f"\nâœ… çˆ¬å–æˆåŠŸ! è€—æ—¶: {duration:.2f}ç§’")
                        print(f"ğŸ“ æ ‡é¢˜: {title}")
                        print(f"ğŸ“… å‘å¸ƒæ—¥æœŸ: {publish_date}")
                        print(f"ğŸ“Š å†…å®¹é•¿åº¦: {content_length}å­—ç¬¦")
                        if ai_score > 0:
                            print(f"ğŸ§  AIéªŒè¯åˆ†æ•°: {ai_score}/100")
                            print(f"ğŸ§  AIéªŒè¯ç†ç”±: {ai_reason}")
                        print(f"ğŸ’¡ å†…å®¹æ‘˜è¦:\n{content_summary}")
                        
                        # æ›´æ–°å†å²é“¾æ¥
                        update_link_history(
                            url=debug_url,
                            title=title,
                            source=debug_source,
                            content_summary=content_summary,
                            is_valid=True,  # è°ƒè¯•æ¨¡å¼ä¸‹å§‹ç»ˆè§†ä¸ºæœ‰æ•ˆ
                            quality_score=ai_score,
                            content_length=content_length,
                            content_fingerprint=content_fingerprint,
                            ai_score=ai_score,
                            ai_reason=ai_reason,
                            link_type=link_type
                        )
                        
                        # å†™å…¥Excelç»“æœ
                        if SAVE_TO_EXCEL:
                            write_result_to_excel(
                                url=debug_url,
                                title=title,
                                source_name=debug_source,
                                publish_date=publish_date,
                                content=content,
                                link_duration=duration,
                                is_valid_content=True,
                                retry_count=0,
                                processed_count=1,
                                ai_score=ai_score,
                                ai_reason=ai_reason,
                                link_type=link_type
                            )
                        
                        # æ‰“å°éƒ¨åˆ†å†…å®¹
                        print("\nğŸ“„ å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦):")
                        print(f"{content[:500]}...")
                        print(f"\nå®Œæ•´å†…å®¹å·²ä¿å­˜åˆ°Excelæ–‡ä»¶: {result_excel_file}")
                        
                    else:
                        print(f"\nâŒ çˆ¬å–å¤±è´¥! è€—æ—¶: {duration:.2f}ç§’")
                        print(f"é”™è¯¯ä¿¡æ¯: {result.error_message}")
                
                except Exception as e:
                    print(f"\nâš ï¸ è°ƒè¯•æ¨¡å¼å‡ºé”™: {str(e)}")
                    traceback.print_exc()
            
            # æ‰§è¡Œå•é“¾æ¥è°ƒè¯•
            asyncio.run(debug_single_link())
            
        else:
            print("âŒ è°ƒè¯•æ¨¡å¼ç”¨æ³•: python test_request3.py --debug <url> <source_name>")
            print("ä¾‹å¦‚: python test_request3.py --debug https://example.com/news/article123 source1")
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šè¿è¡Œå®Œæ•´çˆ¬å–æµç¨‹
        asyncio.run(main()) 